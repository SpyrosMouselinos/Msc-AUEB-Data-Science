{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.8 64-bit ('rapid': conda)",
      "metadata": {
        "interpreter": {
          "hash": "ba59d3597714fba378ce6785a785d823a533603ff6013eacaea4528cdbed07c9"
        }
      }
    },
    "colab": {
      "name": "mura_exersize.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4bgauM6C-X4"
      },
      "source": [
        "### Note: Data has been downloaded, validated and rescaled to 224,224 for images and body parts\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoJMvm0gC-X6"
      },
      "source": [
        "### Imports\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMDTXe6JC-X7"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "### Deep Learning\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.layers.experimental.preprocessing import *\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy, TopKCategoricalAccuracy\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.random import set_seed\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "### Arithmetic\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, cohen_kappa_score\n",
        "from skimage.transform import rescale, resize\n",
        "from sklearn.utils import shuffle as shuffle_iterable\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "### Visuals\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#%matplotlib inline\n",
        "sns.set_style('darkgrid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1jMvMDVC-X8"
      },
      "source": [
        "ROOT_FOLDER = './mura_data/MURA-v1.1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QasW3kvC-X8"
      },
      "source": [
        "### Set Random Seeds ###\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saV4cFVHC-X8"
      },
      "source": [
        "GLOBAL_SEED = 666 # The number of the Beast\n",
        "np.random.seed(GLOBAL_SEED) \n",
        "set_seed(GLOBAL_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWr-G_gPC-X8"
      },
      "source": [
        "def filenames(body_part, train=True):\n",
        "    if train:\n",
        "        csv_path=f\"{ROOT_FOLDER}/train_image_paths_aug.csv\"\n",
        "    else:\n",
        "        csv_path=f\"{ROOT_FOLDER}/valid_image_paths_aug.csv\"\n",
        "    \n",
        "    with open(csv_path, 'rb') as fin:\n",
        "        d = fin.readlines()\n",
        "        if body_part == 'all':\n",
        "            imgs = [ROOT_FOLDER[:-9] + str(x, encoding='utf-8').strip() for x in d]\n",
        "        else:\n",
        "            imgs = [ROOT_FOLDER[:-9] + str(x, encoding='utf-8').strip() for x in d if\n",
        "                            str(x, encoding='utf-8').strip().split('/')[2] == body_part]\n",
        "    labels= [x.split('_')[-1].split('/')[0] for x in imgs]\n",
        "    return imgs, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktg23L0tC-X9"
      },
      "source": [
        "BODY_PARTS = {\n",
        "'XR_ELBOW' : 0,\n",
        "'XR_FINGER' : 1,\n",
        "'XR_FOREARM': 2,\n",
        "'XR_HAND' : 3,\n",
        "'XR_HUMERUS': 4,\n",
        "'XR_SHOULDER': 5,\n",
        "'XR_WRIST': 6\n",
        "}\n",
        "\n",
        "REV_BODY_PARTS = {BODY_PARTS[k]:k for k in BODY_PARTS}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aoVAJV5C-X9"
      },
      "source": [
        "### Explanatory Data Analysis ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O13dwCVEC-X-"
      },
      "source": [
        "### Read the Image files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ntiU-zmC-X-"
      },
      "source": [
        "data = pd.read_csv(f\"{ROOT_FOLDER}/train_image_paths_aug.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Lfv_TJqC-X-"
      },
      "source": [
        "### We will examine imbalance in 3 scenarios\n",
        "* Label imbalance across training set\n",
        "* Body part imbalance across training set\n",
        "* Per Body part Label imbalance across training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjUYjSYWC-X-"
      },
      "source": [
        "pos_neg_count = pd.DataFrame(data['label'].value_counts()).reset_index().sort_values(by='label')\n",
        "posneg_class_weights = compute_class_weight('balanced', classes=[0,1], y=data['label'].apply(lambda x: 1 if x=='positive' else 0).values)\n",
        "\n",
        "body_part_count = pd.DataFrame(data.groupby('part').count()['img_path']).rename(columns={'img_path':'count'}).reset_index().sort_values(by='count')\n",
        "body_part_class_weights = compute_class_weight('balanced', classes=[0,1,2,3,4,5,6], y=data['part'].values)\n",
        "\n",
        "per_body_part_pos_neg_count = pd.DataFrame(data.groupby('part')['label'].value_counts()).rename(columns={'label':'count'}).reset_index()\n",
        "\n",
        "XR_ELBOW_pos_neg = compute_class_weight('balanced',    classes=[0,1], y=data[data['part'] == 0]['label'].apply(lambda x: 1 if x=='positive' else 0).values)\n",
        "XR_FINGER_pos_neg = compute_class_weight('balanced',   classes=[0,1], y=data[data['part'] == 1]['label'].apply(lambda x: 1 if x=='positive' else 0).values)\n",
        "XR_FOREARM_pos_neg = compute_class_weight('balanced',  classes=[0,1], y=data[data['part'] == 2]['label'].apply(lambda x: 1 if x=='positive' else 0).values)\n",
        "XR_HAND_pos_neg = compute_class_weight('balanced',     classes=[0,1], y=data[data['part'] == 3]['label'].apply(lambda x: 1 if x=='positive' else 0).values)\n",
        "XR_HUMERUS_pos_neg = compute_class_weight('balanced',  classes=[0,1], y=data[data['part'] == 4]['label'].apply(lambda x: 1 if x=='positive' else 0).values)\n",
        "XR_SHOULDER_pos_neg = compute_class_weight('balanced', classes=[0,1], y=data[data['part'] == 5]['label'].apply(lambda x: 1 if x=='positive' else 0).values)\n",
        "XR_WRIST_pos_neg = compute_class_weight('balanced',    classes=[0,1], y=data[data['part'] == 6]['label'].apply(lambda x: 1 if x=='positive' else 0).values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef5rh14tC-X_"
      },
      "source": [
        "### Store imbalance score per part\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhDujm-wC-X_"
      },
      "source": [
        "BODY_PART_IMB_SCORE = {\n",
        "    'XR_ELBOW' :   {0:XR_ELBOW_pos_neg[0],    1:XR_ELBOW_pos_neg[1]},\n",
        "    'XR_FINGER' :  {0:XR_FINGER_pos_neg[0],   1:XR_FINGER_pos_neg[1]},\n",
        "    'XR_FOREARM':  {0:XR_FOREARM_pos_neg[0],  1:XR_FOREARM_pos_neg[1]},\n",
        "    'XR_HAND' :    {0:XR_HAND_pos_neg[0],     1:XR_HAND_pos_neg[1]},\n",
        "    'XR_HUMERUS':  {0:XR_HUMERUS_pos_neg[0],  1:XR_HUMERUS_pos_neg[1]},\n",
        "    'XR_SHOULDER': {0:XR_SHOULDER_pos_neg[0], 1:XR_SHOULDER_pos_neg[1]},\n",
        "    'XR_WRIST':    {0:XR_WRIST_pos_neg[0],    1:XR_WRIST_pos_neg[1]}\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQji2IH2C-YA",
        "outputId": "b55a103c-8da6-47d9-f9ad-69be34716e2a"
      },
      "source": [
        "BODY_PART_IMB_SCORE['XR_HUMERUS']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXsRftcVC-YB",
        "outputId": "0b172aed-5715-4ee1-a820-6ba7a9d82e11"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.title(\"Global Label Imbalance\")\n",
        "_ =sns.barplot(x=\"index\", y=\"label\", data=pos_neg_count)\n",
        "plt.ylabel(\"Number of Examples\")\n",
        "plt.xlabel(None)\n",
        "plt.xticks(ticks=[0,1], labels=['Positive', 'Negative'])\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf_Cb2UqC-YB",
        "outputId": "fce0e603-82a0-4f13-a12e-0870f64a486c"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.title(\"Example Imbalance per Body Part\")\n",
        "_ =sns.barplot(x=\"part\", y=\"count\", data=body_part_count, order=body_part_count.part)\n",
        "plt.ylabel(\"Number of Examples\")\n",
        "plt.xlabel(None)\n",
        "plt.xticks(ticks=[f for f,_ in enumerate(REV_BODY_PARTS)], labels=[REV_BODY_PARTS[f] for f in body_part_count['part']])\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEUyYJLWC-YB",
        "outputId": "39b7e28f-7957-4b10-8533-fa671cbcad48"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.title(\"Label imbalance per Body Part\")\n",
        "ax = sns.barplot(x=\"part\", y=\"count\", hue=\"label\", data=per_body_part_pos_neg_count, order=body_part_count.part)\n",
        "plt.ylabel(\"Number of Examples\")\n",
        "plt.xlabel(None)\n",
        "plt.xticks(ticks=[f for f,_ in enumerate(REV_BODY_PARTS)], labels=[REV_BODY_PARTS[f] for f in body_part_count['part']])\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1ldJ5w6C-YB"
      },
      "source": [
        "### Exersize Description\n",
        "\n",
        "**Task**: Given a set of diagnostic images of a body part classify if there is a disease or not\n",
        "\n",
        "**Data**:\n",
        "- The data are grayscale images of varying size (usually (400-500) x (400-500))\n",
        "\n",
        "**Approaches**\n",
        "- According to the exersize the first approach would be to use a hand crafted CNN to classify the images.\n",
        "- The second approach would be using a pretrained CNN for the same task.\n",
        "\n",
        "\n",
        "**Preprocessing**\n",
        "- Images will be normalized to MinMax or Imagenet scale.\n",
        "- The effect of image augmentations will be tested as well.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8L9vlKNC-YC"
      },
      "source": [
        "def split_stratified_into_train_val_test(df_input, stratify_colname,\n",
        "                                         frac_train, frac_val,\n",
        "                                         random_state=None, recombine=True):\n",
        "    '''\n",
        "    Splits a Pandas dataframe into two subsets (train, val)\n",
        "    following fractional ratios provided by the user, where each subset is\n",
        "    stratified by the values in a specific column (that is, each subset has\n",
        "    the same relative frequency of the values in the column).\n",
        "    '''\n",
        "\n",
        "    if frac_train + frac_val  != 1.0:\n",
        "        raise ValueError('fractions %f, %f, %f do not add up to 1.0' % \\\n",
        "                         (frac_train, frac_val, frac_test))\n",
        "\n",
        "    if stratify_colname not in df_input.columns:\n",
        "        raise ValueError('%s is not a column in the dataframe' % (stratify_colname))\n",
        "\n",
        "    X = df_input # Contains all columns.\n",
        "    y = df_input[[stratify_colname]] # Dataframe of just the column on which to stratify.\n",
        "\n",
        "    # Split original dataframe into train and temp dataframes.\n",
        "    df_train, df_val, y_train, y_val = train_test_split(X,\n",
        "                                                          y,\n",
        "                                                          stratify=y,\n",
        "                                                          test_size=(1.0 - frac_train),\n",
        "                                                          random_state=random_state)\n",
        "\n",
        "    assert len(df_input) == len(df_train) + len(df_val)\n",
        "\n",
        "    if recombine:\n",
        "        return pd.concat([df_train,df_val],axis=0)\n",
        "    return df_train, df_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLCM2QQ0C-YC"
      },
      "source": [
        "### Experiment 1: Focus on a specific Body Part\n",
        "#### Note 1: Use the MobilNet_v2 architecture since it is lightweight and accurate\n",
        "#### Note 2: We will use part of the training images as validation set, and the given validation set as test set\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPopZMIDC-YC"
      },
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYVdMFkXC-YC"
      },
      "source": [
        "class Patches(tf.keras.layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQON2DLtC-YD"
      },
      "source": [
        "class PatchEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zXViCQ9C-YD"
      },
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 8\n",
        "num_epochs = 100\n",
        "image_size = 224  \n",
        "patch_size = 16  \n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "] \n",
        "transformer_layers = 4\n",
        "mlp_head_units = [256, 128]\n",
        "\n",
        "def create_vit_classifier():\n",
        "    image_input = Input((224,224,3), name='CustomInputLayer')\n",
        "    x = tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\")(image_input)\n",
        "    x = tf.keras.layers.experimental.preprocessing.RandomRotation(0.05)(x)\n",
        "    x = tf.keras.layers.experimental.preprocessing.RandomZoom(-0.1, 0.1)(x)\n",
        "    x = tf.keras.layers.experimental.preprocessing.RandomHeight(0.01)(x)\n",
        "    x = tf.keras.layers.experimental.preprocessing.RandomWidth(0.01)(x)\n",
        "    x = tf.keras.layers.experimental.preprocessing.RandomContrast(0.2)(x)\n",
        "    x = tf.keras.layers.experimental.preprocessing.Resizing(224,224)(x)\n",
        "    x = Patches(patch_size=16)(x)\n",
        "    encoded_patches = PatchEncoder(num_patches=14, projection_dim=64)(x)\n",
        "\n",
        "\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = Flatten()(representation)\n",
        "    representation = Dropout(0.5)(representation)\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    decision = Dense(units=1, activation='sigmoid')(features)\n",
        "    body_part = Dense(units=7, activation='softmax')(features)\n",
        "    model = Model(inputs=inputs, outputs=[decision, body_part])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpOgxZMC-YD"
      },
      "source": [
        "### I would like to use Residual Blocks so i will implement a simple residual block here\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3szT3bKC-YE"
      },
      "source": [
        "def residual_block(x, downsample, filters, kernel_size):\n",
        "    y = Conv2D(kernel_size=kernel_size,\n",
        "               strides= (1 if not downsample else 2),\n",
        "               filters=filters,\n",
        "               padding=\"same\")(x)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = ReLU(negative_slope=0.1)(y)\n",
        "    y = Conv2D(kernel_size=kernel_size,\n",
        "               strides=1,\n",
        "               filters=filters,\n",
        "               padding=\"same\")(y)\n",
        "\n",
        "    if downsample:\n",
        "        x = Conv2D(kernel_size=1,\n",
        "                   strides=2,\n",
        "                   filters=filters,\n",
        "                   padding=\"same\")(x)\n",
        "    out = Add()([x, y])\n",
        "    out = BatchNormalization()(out)\n",
        "    out = ReLU(negative_slope=0.1)(out)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz4gQVgIC-YE"
      },
      "source": [
        "### This is a standard convolutional block\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j--KucKyC-YE"
      },
      "source": [
        "def plain_block(x, pool, filters, kernel_size, use_bn, use_dr, strides):\n",
        "    x = Conv2D(kernel_size=kernel_size, strides= strides, filters=filters, padding=\"same\")(x)\n",
        "    if use_bn:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = ReLU(negative_slope=0.1)(x)\n",
        "    if use_dr > 0:\n",
        "        x = Dropout(use_dr)(x)\n",
        "    if pool > 0:\n",
        "        x = MaxPool2D(pool_size=(pool,pool), strides=(pool,pool))(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vg2SsaKC-YE"
      },
      "source": [
        "### Added Weighted Binary Crossentropy to care for class imbalance\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecSQQp1xC-YF"
      },
      "source": [
        "def weighted_bincrossentropy(weights):\n",
        "    weight_zero = tf.cast(weights[0], tf.float32)\n",
        "    weight_one = tf.cast(weights[1], tf.float32)\n",
        "    def _weighted_bincrossentropy(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = tf.cast(y_pred, tf.float32)\n",
        "        bin_crossentropy = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n",
        "        weights_ = y_true * weight_one + (1. - y_true) * weight_zero\n",
        "        weighted_bin_crossentropy = weights_ * bin_crossentropy \n",
        "        return tf.keras.backend.mean(weighted_bin_crossentropy)\n",
        "    return _weighted_bincrossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTXbIb0LC-YF"
      },
      "source": [
        "def run_experiment(body_part, mode='pretrained'):\n",
        "    experiment_name = body_part\n",
        "\n",
        "    ### Gather the dataframe of the training images\n",
        "    train_dataframe = pd.read_csv(ROOT_FOLDER + '/train_image_paths_aug.csv')\n",
        "    train_dataframe = train_dataframe[train_dataframe['part'] == BODY_PARTS[experiment_name]]\n",
        "    train_dataframe['label'] = train_dataframe['label'].apply(lambda x: 1 if x=='positive' else 0)\n",
        "\n",
        "\n",
        "    ### Shuffle it before passing to a generator so classes are stratified\n",
        "    if mode=='pretrained':\n",
        "        train_dataframe = split_stratified_into_train_val_test(train_dataframe,stratify_colname='label',frac_train=0.9,frac_val=0.1,random_state=GLOBAL_SEED, recombine=True)\n",
        "    else:\n",
        "        train_dataframe = split_stratified_into_train_val_test(train_dataframe,stratify_colname='label',frac_train=0.9,frac_val=0.1,random_state=GLOBAL_SEED, recombine=True)\n",
        "\n",
        "    ### Gather the dataframe of the test images\n",
        "    test_dataframe = pd.read_csv(ROOT_FOLDER + '/valid_image_paths_aug.csv')\n",
        "    test_dataframe = test_dataframe[test_dataframe['part'] == BODY_PARTS[experiment_name]]\n",
        "    test_dataframe['label'] = test_dataframe['label'].apply(lambda x: 1 if x=='positive' else 0)\n",
        "\n",
        "    ### Calculate the Dummy Baseline\n",
        "    \n",
        "    train_labels = train_dataframe['label'].values\n",
        "    train_dummy_input = np.zeros_like(train_labels)\n",
        "\n",
        "    test_labels = test_dataframe['label'].values\n",
        "    test_dummy_input = np.zeros_like(test_labels)\n",
        "\n",
        "    clf = DummyClassifier().fit(train_dummy_input, train_labels)\n",
        "    dummy_train_prediction = clf.predict(train_dummy_input)\n",
        "    dummy_test_prediction = clf.predict(test_dummy_input)\n",
        "\n",
        "    print(f\"Part: {experiment_name}\")\n",
        "    base_train_acc = accuracy_score(train_labels, dummy_train_prediction)\n",
        "    base_cohen_score = cohen_kappa_score(train_labels, dummy_train_prediction)\n",
        "    \n",
        "    print(f\"Baseline Train / Val Results: {base_train_acc} / {base_cohen_score}\")\n",
        "\n",
        "    base_test_acc = accuracy_score(test_labels, dummy_test_prediction)\n",
        "    base_test_cohen_score = cohen_kappa_score(test_labels, dummy_test_prediction)\n",
        "    print(f\"Baseline Test Results: {base_test_acc} / {base_test_cohen_score}\")\n",
        "\n",
        "    # Base train / validation image generator\n",
        "    if mode=='pretrained':\n",
        "        datagen = ImageDataGenerator(\n",
        "            validation_split=0.1,\n",
        "            preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n",
        "            )\n",
        "\n",
        "        # Base test image generator\n",
        "        datagen2 = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input)\n",
        "    else:\n",
        "        datagen = ImageDataGenerator(\n",
        "            validation_split=0.1,\n",
        "            rescale=1/255.0,\n",
        "            )\n",
        "\n",
        "        # Base test image generator\n",
        "        datagen2 = ImageDataGenerator(rescale=1/255.0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    train_generator = datagen.flow_from_dataframe(\n",
        "        dataframe = train_dataframe,\n",
        "        directory= ROOT_FOLDER[:-9],\n",
        "        x_col = \"img_path\",\n",
        "        y_col = \"label\",\n",
        "        batch_size=32,\n",
        "        seed=GLOBAL_SEED,\n",
        "        shuffle=True,\n",
        "        subset=\"training\",\n",
        "        validate_filenames=False,\n",
        "        class_mode=\"raw\",\n",
        "        target_size=(224,224))\n",
        "\n",
        "\n",
        "    validation_generator = datagen.flow_from_dataframe(\n",
        "        dataframe = train_dataframe,\n",
        "        directory= ROOT_FOLDER[:-9],\n",
        "        x_col = \"img_path\",\n",
        "        y_col = \"label\",\n",
        "        batch_size=32,\n",
        "        seed=GLOBAL_SEED,\n",
        "        shuffle=False,\n",
        "        subset=\"validation\",\n",
        "        validate_filenames=False,\n",
        "        class_mode=\"raw\",\n",
        "        target_size=(224,224))\n",
        "\n",
        "    test_generator = datagen2.flow_from_dataframe(\n",
        "        dataframe = test_dataframe,\n",
        "        directory= ROOT_FOLDER[:-9],\n",
        "        x_col = \"img_path\",\n",
        "        y_col = \"label\",\n",
        "        batch_size=32,\n",
        "        seed=GLOBAL_SEED,\n",
        "        shuffle=False,\n",
        "        validate_filenames=False,\n",
        "        class_mode=\"raw\",\n",
        "        target_size=(224,224))\n",
        "\n",
        "    ### Set the Training Callbacks\n",
        "    es = EarlyStopping(monitor='val_accuracy', verbose=0, patience=20, restore_best_weights=True)\n",
        "    mc = ModelCheckpoint(filepath=f'./{experiment_name}/{mode}_2',monitor='val_accuracy', mode='max', save_best_only=True)\n",
        "    rl = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=10, min_delta=0.001, verbose=1, min_lr=1e-8, cooldown=5)\n",
        "    tb = TensorBoard(f'./{experiment_name}/logs/{mode}_2', update_freq=1)\n",
        "    callbacks = [es,rl]\n",
        "\n",
        "\n",
        "    ### Get a pre-trained MobileNetV2 Model (or make a simple one from scratch) and the CohenKappa Metric\n",
        "    if mode == 'pretrained':\n",
        "        PreTrainedBase = tf.keras.applications.MobileNetV2(include_top=False, input_shape=(224,224,3))\n",
        "    cohen_metric = tfa.metrics.CohenKappa(num_classes=2, sparse_labels=True)\n",
        "\n",
        "\n",
        "    if mode == 'pretrained':\n",
        "        # Freeze them up to the non-fc top layer\n",
        "        for layer in PreTrainedBase.layers[:-1]:\n",
        "            layer.trainable = False\n",
        "\n",
        "\n",
        "    image_input = Input((224,224,3), name='CustomInputLayer')\n",
        "    x = tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\")(image_input)\n",
        "    x = tf.keras.layers.experimental.preprocessing.RandomRotation(0.05)(x)\n",
        "    x = tf.keras.layers.experimental.preprocessing.RandomZoom(-0.1, 0.1)(x)\n",
        "    x = tf.keras.layers.experimental.preprocessing.RandomHeight(0.01)(x)\n",
        "    x = tf.keras.layers.experimental.preprocessing.RandomWidth(0.01)(x)\n",
        "    x = tf.keras.layers.experimental.preprocessing.RandomContrast(0.2)(x)\n",
        "    x = tf.keras.layers.experimental.preprocessing.Resizing(224,224)(x)\n",
        "\n",
        "    ### Up to this point the layers would have been the same so differentiate the main model here\n",
        "    if mode == 'pretrained':\n",
        "        x = PreTrainedBase(x)\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(units=128, activation='relu')(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        model_output = Dense(units=1, activation='sigmoid')(x)\n",
        "        model= Model(inputs=image_input,outputs=model_output, name='all_model')\n",
        "    else:\n",
        "        ### Use my custom network\n",
        "        x = plain_block(x, pool=2, filters=32,  kernel_size=(3,3),   strides=(1,1),  use_bn=True,   use_dr=0)\n",
        "        x = plain_block(x, pool=2, filters=64,  kernel_size=(3,3),   strides=(1,1),  use_bn=False,   use_dr=0)\n",
        "        x = plain_block(x, pool=2, filters=64,  kernel_size=(3,3),   strides=(1,1),  use_bn=True,   use_dr=0)\n",
        "        x = plain_block(x, pool=2, filters=128,  kernel_size=(3,3),   strides=(1,1),  use_bn=False,   use_dr=0)\n",
        "\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(units=128, activation='relu')(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        model_output = Dense(units=1, activation='sigmoid')(x)\n",
        "        model= Model(inputs=image_input,outputs=model_output, name='all_model')\n",
        "\n",
        "    model.compile(optimizer=Adam(lr=1e-4), loss=weighted_bincrossentropy(weights=BODY_PART_IMB_SCORE[experiment_name]), metrics=['accuracy', cohen_metric])\n",
        "    model.summary()\n",
        "\n",
        "    history = model.fit(train_generator,\n",
        "                    steps_per_epoch = len(train_generator),\n",
        "                    epochs = 30,\n",
        "                    validation_data = validation_generator,\n",
        "                    validation_steps = len(validation_generator),\n",
        "                    callbacks=callbacks, workers=4, max_queue_size=20, use_multiprocessing=False)\n",
        "    model.save(f'./{experiment_name}/{mode}_2')\n",
        "\n",
        "    ### Write a Test function to update our logs\n",
        "    def update_results(model, path, split='test'):\n",
        "        if split == 'train':\n",
        "            gen = train_generator\n",
        "        elif split == 'valid':\n",
        "            gen = validation_generator\n",
        "        else:\n",
        "            gen = test_generator\n",
        "        l,a,c =model.evaluate(gen)\n",
        "        if os.path.exists(path+f'/{split}_results.txt'):\n",
        "            os.remove(path+f'/{split}_results.txt')\n",
        "        with open(path+f'/{split}_results.txt', 'w') as fout:\n",
        "            fout.write(f\"{split} Loss,{split} Accuracy,{split} Cohen\\n{l},{a},{c}\")\n",
        "        return\n",
        "\n",
        "    ### Use it\n",
        "    update_results(model, path=f'./{experiment_name}/logs/{mode}_2', split='test')\n",
        "    update_results(model, path=f'./{experiment_name}/logs/{mode}_2', split='valid')\n",
        "    update_results(model, path=f'./{experiment_name}/logs/{mode}_2', split='train') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVzBd2URC-YF"
      },
      "source": [
        "# for i, body_part in enumerate(BODY_PARTS):\n",
        "#     run_experiment(body_part, 'pretrained')\n",
        "#     print()\n",
        "#     print()\n",
        "#     print(f\"Finished {i+1}/7\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krj8ro7RC-YF",
        "outputId": "3f211b61-7b86-4e62-da9a-730cefa06a18"
      },
      "source": [
        "for i, body_part in enumerate(BODY_PARTS):\n",
        "    run_experiment(body_part, 'custom')\n",
        "    print()\n",
        "    print()\n",
        "    print(f\"Finished {i+1}/7\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYkIGbJMC-YG"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mUzQTqhC-YG"
      },
      "source": [
        "### Write a function to clean up logs in case something bad happens\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSO8CLmyC-YG"
      },
      "source": [
        "def clean_logs(path):\n",
        "    if os.path.exists(path):\n",
        "        shutil.rmtree(path)\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azn1diMiC-YG"
      },
      "source": [
        "### Perform Experiments with Loaded Models for Reproducability\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af1af5XpC-YG"
      },
      "source": [
        "def run_test_experiment(body_part, mode):\n",
        "    ### Gather the dataframe of the test images\n",
        "    test_dataframe = pd.read_csv(ROOT_FOLDER + '/valid_image_paths_aug.csv')\n",
        "    test_dataframe = test_dataframe[test_dataframe['part'] == BODY_PARTS[body_part]]\n",
        "    test_dataframe['label'] = test_dataframe['label'].apply(lambda x: 1 if x=='positive' else 0)\n",
        "    if mode == 'custom':\n",
        "        datagen2 = ImageDataGenerator(rescale=1/255.0)\n",
        "    else:\n",
        "        datagen2 = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input)\n",
        "\n",
        "    test_generator = datagen2.flow_from_dataframe(\n",
        "        dataframe = test_dataframe,\n",
        "        directory= ROOT_FOLDER[:-9],\n",
        "        x_col = \"img_path\",\n",
        "        y_col = \"label\",\n",
        "        batch_size=32,\n",
        "        seed=GLOBAL_SEED,\n",
        "        shuffle=False,\n",
        "        validate_filenames=False,\n",
        "        class_mode=\"raw\",\n",
        "        target_size=(224,224))\n",
        "\n",
        "    # Initialize the custom loss with balanced weights (it does not matter we are not going to train...)\n",
        "    model = tf.keras.models.load_model(F'./{body_part}/{mode}', custom_objects={'_weighted_bincrossentropy': weighted_bincrossentropy([1,1])})\n",
        "    # Predict per image\n",
        "    l, a, c  = model.evaluate_generator(test_generator)\n",
        "    with open(f'./{body_part}_{mode}_per_image.txt', 'w') as fout:\n",
        "        fout.write(f\"Test Loss, Test Accuracy, Test Cohen\\n{l},{a},{c}\")\n",
        "    \n",
        "    # Predict per_study\n",
        "    predictions = model.predict(test_generator)\n",
        "    predictions  = np.squeeze(predictions)\n",
        "    test_dataframe['prediction'] = predictions\n",
        "    test_dataframe['patient'] = test_dataframe['img_path'].apply(lambda x: x.split('/')[3])\n",
        "    test_dataframe['study'] = test_dataframe['img_path'].apply(lambda x: x.split('/')[4].split('_')[0])\n",
        "    study_dataframe = test_dataframe.groupby(['patient','study']).mean('prediction')\n",
        "    study_dataframe['prediction'] = study_dataframe['prediction'].apply(lambda x: 1 if x > 0.499 else 0)\n",
        "    y_true = study_dataframe['label'].values\n",
        "    y_pred = study_dataframe['prediction'].values\n",
        "    study_a = accuracy_score(y_true, y_pred)\n",
        "    study_c = cohen_kappa_score(y_true, y_pred)\n",
        "    with open(f'./{body_part}_{mode}_per_study.txt', 'w') as fout:\n",
        "        fout.write(f\"Test Accuracy, Test Cohen\\n{study_a},{study_c}\")\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFRb6Zd8C-YG"
      },
      "source": [
        "# for i, body_part in enumerate(BODY_PARTS):\n",
        "#     if body_part == 'XR_WRIST':\n",
        "#      run_test_experiment(body_part, mode='pretrained')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6k25USfC-YH",
        "outputId": "3c16a2e6-49a2-42bb-f605-d708fa719816"
      },
      "source": [
        "for i, body_part in enumerate(BODY_PARTS):\n",
        "    run_test_experiment(body_part, mode='custom')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3350gRA_C-YH"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}