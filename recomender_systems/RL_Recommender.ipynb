{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for EDA and Q Learning\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from math import log2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "from itertools import combinations\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from collections import Counter\n",
    "import yellowbrick\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer, InterclusterDistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Clicks File\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df = pd.read_csv('../data/yoochoose-clicks.dat', \n",
    "    delimiter=',',\n",
    "    usecols=[0,2],\n",
    "    names=['session_id','item_id'],\n",
    "    low_memory=True,\n",
    "    header=None,\n",
    "    dtype={\n",
    "        'session_id': 'uint32',\n",
    "        'item_id': 'uint32',\n",
    "    }             \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Memory usage: \\n',clicks_df.info(memory_usage=\"deep\"))\n",
    "print('\\n''Top 5 rows: \\n',clicks_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_sessions = clicks_df['session_id'].unique()\n",
    "print(len(initial_sessions))\n",
    "initial_items = clicks_df['item_id'].unique()\n",
    "print(len(initial_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only sessions that include the top k occuring products\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_items = clicks_df['item_id'].value_counts()[0:k]\n",
    "topk_items =  pd.DataFrame(data=list(topk_items.index), columns=['item_id'])\n",
    "clicks_df = pd.merge(topk_items, clicks_df, how='left', on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_frame = clicks_df.groupby('session_id').count()\n",
    "duration_frame = duration_frame[(duration_frame['item_id'] >=4) & (duration_frame['item_id'] <=10)]\n",
    "clicks_df = pd.merge(duration_frame, clicks_df, on='session_id', how='left')\n",
    "clicks_df = clicks_df[['session_id', 'item_id_y']].rename(columns={'item_id_y':'item_id'})\n",
    "clicks_df.reset_index(inplace=True, drop=True)\n",
    "del duration_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df.to_csv(f'../data/yoochoose-clicks-compressed-top{k}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df = pd.read_csv(f'../data/yoochoose-clicks-compressed-top{k}.csv')\n",
    "clicks_df['item_id'] = clicks_df['item_id'].astype('uint16')\n",
    "clicks_df['session_id'] = clicks_df['session_id'].astype('uint32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_sessions = clicks_df['session_id'].unique()\n",
    "print(len(initial_sessions))\n",
    "initial_items = clicks_df['item_id'].unique()\n",
    "print(len(initial_items))"
   ]
  },
  {
   "source": [
    "### Cluster Users by Activity\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_as_clicks = clicks_df.groupby('session_id')['item_id'].apply(list).reset_index(name='item_id_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_as_clicks['item_id_list'] = users_as_clicks['item_id_list'].apply(lambda x: ' '.join([str(f) for f in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "x = vectorizer.fit_transform(users_as_clicks['item_id_list'])\n",
    "reverse_bow_dict = {vectorizer.vocabulary_[k]:k for k in vectorizer.vocabulary_.keys()}"
   ]
  },
  {
   "source": [
    "### Find the best Number of Clusters with Elbow Method\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(data_to_cluster):\n",
    "    kmeans = MiniBatchKMeans(random_state=1453,  batch_size=2048, max_iter=1000)\n",
    "    visualizer = KElbowVisualizer(kmeans, k=(10,21))\n",
    "    visualizer.fit(data_to_cluster)\n",
    "    visualizer.show()\n",
    "    return\n",
    "\n",
    "def top_k(cluster_center, k=10, lexicon=None):\n",
    "    if lexicon is None:\n",
    "        print(\"Pass something to reverse engineer!\\n\")\n",
    "        return\n",
    "    \n",
    "    cluster_idx = (-cluster_center).argsort()[:k]\n",
    "    words_of_importance = [lexicon[f] for f in cluster_idx]\n",
    "    return words_of_importance\n",
    "\n",
    "\n",
    "def inspect_clusters(model,data_to_cluster,reverse_dict,n_clusters):\n",
    "    # Instantiate the clustering model and visualizer\n",
    "    \n",
    "    visualizer = InterclusterDistance(model, size=(1080, 720), legend_locstr='upper left', random_state=1453)\n",
    "    visualizer.fit(data_to_cluster)   \n",
    "    _ = visualizer.show()        \n",
    "\n",
    "    for i, cluster_center in enumerate(model.cluster_centers_):\n",
    "        print(\"Cluster {} most important dimension indexes...\".format(i))\n",
    "        print(top_k(cluster_center, lexicon=reverse_dict))\n",
    "        print()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = perform_clustering(data_to_cluster = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if k==100:\n",
    "    best_cluster = MiniBatchKMeans(n_clusters=13, random_state=1453,  batch_size=2048, max_iter=2000).fit(x)\n",
    "elif k==500:\n",
    "    best_cluster = MiniBatchKMeans(n_clusters=13, random_state=1453,  batch_size=2048, max_iter=2000).fit(x)\n",
    "elif k==1000:\n",
    "    best_cluster = MiniBatchKMeans(n_clusters=13, random_state=1453,  batch_size=2048, max_iter=2000).fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_clusters(best_cluster, x, reverse_dict=reverse_bow_dict, n_clusters=13)"
   ]
  },
  {
   "source": [
    "### We pick a cluster that is relatively disjoint and contains enough data\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignment = best_cluster.predict(x)\n",
    "users_as_clicks['cluster'] = cluster_assignment\n",
    "if k == 100:\n",
    "    users_as_clicks = users_as_clicks[users_as_clicks['cluster'] == 12]\n",
    "elif k == 500:\n",
    "    users_as_clicks = users_as_clicks[users_as_clicks['cluster'] == 7]\n",
    "elif k == 1000:\n",
    "    users_as_clicks = users_as_clicks[users_as_clicks['cluster'] == 12]\n",
    "\n",
    "clicks_df = pd.merge(clicks_df, users_as_clicks['session_id'], how='right', on='session_id')\n",
    "\n",
    "item_dict = {v:i for i,v in enumerate(set(clicks_df['item_id'].values))}\n",
    "session_dict = {v:i for i,v in enumerate(set(clicks_df['session_id'].values))}\n",
    "\n",
    "clicks_df['session_id'] = clicks_df['session_id'].apply(lambda x: session_dict[x])\n",
    "clicks_df['item_id'] = clicks_df['item_id'].apply(lambda x: item_dict[x])\n",
    "clicks_df['item_id'] = clicks_df['item_id'].astype('uint16')\n",
    "clicks_df['session_id'] = clicks_df['session_id'].astype('uint32')\n",
    "n_sessions = clicks_df['session_id'].max()\n",
    "max_q_size = clicks_df['item_id'].max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_sessions = clicks_df['session_id'].unique()\n",
    "print(len(initial_sessions))\n",
    "initial_items = clicks_df['item_id'].unique()\n",
    "print(len(initial_items))"
   ]
  },
  {
   "source": [
    "### Split into Train / Test\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_RATIO = 0.8\n",
    "SPLIT_SESSION = round(SPLIT_RATIO * n_sessions - 1)\n",
    "train_click_df = clicks_df[clicks_df['session_id'] <= SPLIT_SESSION]\n",
    "test_click_df = clicks_df[clicks_df['session_id'] > SPLIT_SESSION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_train_sessions = train_click_df['session_id'].unique()\n",
    "print(len(initial_train_sessions))\n",
    "initial_train_items = train_click_df['item_id'].unique()\n",
    "print(len(initial_train_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_test_sessions = test_click_df['session_id'].unique()\n",
    "print(len(initial_test_sessions))\n",
    "initial_test_items = test_click_df['item_id'].unique()\n",
    "print(len(initial_test_items))"
   ]
  },
  {
   "source": [
    "### Environment in OpenAI Format\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MooChoose_v1(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df, window_size=1, possible_items=90):\n",
    "        self.window_size = window_size\n",
    "        self.df = df\n",
    "        self.current_step = None\n",
    "        self.session_df = None\n",
    "\n",
    "        self.current_session = self.min_session = self.df['session_id'].min()\n",
    "        self.max_session = self.df['session_id'].max()\n",
    "\n",
    "        self.n_sessions = self.max_session - self.min_session \n",
    "\n",
    "        self.n_items = possible_items\n",
    "        self.action_space = spaces.Discrete(self.n_items)\n",
    "        self.observation_space = spaces.Discrete(self.n_items)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step +=1\n",
    "        state = self.session_df.loc[ self.current_step : self.current_step+(self.window_size-1),'item_id']\n",
    "        if state.values[-1] == action:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            \n",
    "        # Game is done after a step out of bounds\n",
    "        if self.current_step >= len(self.session_df)- self.window_size:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return state.values, reward, done, info\n",
    "\n",
    "    def reset(self, debug=False):\n",
    "        if debug:\n",
    "            self.current_session = self.min_session\n",
    "        else:\n",
    "            self.current_session = self.min_session + ((self.current_session + 1) % self.n_sessions)\n",
    "\n",
    "        # Get the data points from the current session\n",
    "        self.session_df = self.df.loc[self.df['session_id'] == self.current_session, ['item_id']].reset_index(drop=True)\n",
    "        # initialize step to get the first 2 rows\n",
    "        self.current_step = 0\n",
    "        return self.session_df.loc[self.current_step : self.current_step +(self.window_size-1),'item_id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Table initialization and Q Table Warmup Function\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_q(s, a, type=\"ones\"):\n",
    "    \"\"\"\n",
    "    @param s the number of states\n",
    "    @param a the number of actions\n",
    "    @param type random, ones or zeros for the initialization\n",
    "    \"\"\"\n",
    "    if type == \"ones\":\n",
    "        return np.ones((s, a), dtype='float16')\n",
    "    elif type == \"random\":\n",
    "        return np.random.random((s, a), dtype='float16')\n",
    "    elif type == \"zeros\":\n",
    "        return np.zeros((s, a), dtype='float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_warmup(frame, percentage=0.7, max_q_size=295):\n",
    "\n",
    "    split = round(percentage * frame['session_id'].max() - 1)\n",
    "    warmup_df = frame[frame['session_id'] < split].copy()\n",
    "\n",
    "    train_click_df = frame[frame['session_id'] >= split].copy()\n",
    "\n",
    "\n",
    "    grouped_consecutive_product_ids = warmup_df.groupby(['session_id'])['item_id'].apply(lambda x: [tuple(sorted(pair)) for pair in zip(x,x[1:])]).reset_index()\n",
    "    df1=pd.DataFrame(grouped_consecutive_product_ids)\n",
    "    s=df1.item_id.apply(lambda x: pd.Series(x)).unstack()\n",
    "    df2=pd.DataFrame(s.reset_index(level=0,drop=True)).dropna()\n",
    "    df2.rename(columns = {0:'Bigram'}, inplace = True)\n",
    "    df2[\"freq\"] = df2.groupby('Bigram')['Bigram'].transform('count')\n",
    "    bigram_frequency_consecutive = df2.drop_duplicates(keep=\"first\").sort_values(\"Bigram\").reset_index()\n",
    "    del bigram_frequency_consecutive[\"index\"]\n",
    "\n",
    "    Q_bayes = init_q(max_q_size, max_q_size, type=\"zeros\")\n",
    "    for _, t in bigram_frequency_consecutive.iterrows():\n",
    "        indexing = t[0]\n",
    "        freq = t[1]\n",
    "        Q_bayes[indexing] = freq\n",
    "    return Q_bayes, train_click_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q, epsilon, n_actions, s, exploit_only=False):\n",
    "    \"\"\"\n",
    "    @param Q Q values state x action -> value\n",
    "    @param epsilon for exploration\n",
    "    @param s number of states\n",
    "    @param exploit_only if true then no random actions selected\n",
    "    \"\"\"\n",
    "    if exploit_only:\n",
    "        # Return the best 10 actions\n",
    "        action = np.argsort(Q[s, :])[::-10]\n",
    "    elif np.random.rand() > epsilon:\n",
    "        action = np.argmax(Q[s, :])\n",
    "    else:\n",
    "        action = np.random.randint(0, n_actions)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearning(env, Q,  \n",
    "                alpha=1,\n",
    "                gamma=0.5, \n",
    "                episodes=50_000, \n",
    "                max_steps=10, \n",
    "                exploration_decay_rate=0.005, \n",
    "                exploit_only=False):\n",
    "    \"\"\"\n",
    "    @param alpha learning rate\n",
    "    @param gamma decay factor\n",
    "    @param epsilon for exploration\n",
    "    @param max_steps for max step in each episode\n",
    "    @param n_tests number of test episodes\n",
    "    \"\"\"\n",
    "    timestep_reward = []\n",
    "    # Hit Rate Metrics # \n",
    "    hr_at_5 = 0\n",
    "    hr_at_10 = 0\n",
    "\n",
    "    # NDCG Metrics #\n",
    "    ndcg_at_5 = 0\n",
    "    ndcg_at_10 = 0\n",
    "    # Engagement Metric #\n",
    "    eng = 0\n",
    "\n",
    "    epsilon = 1\n",
    "    n_states, n_actions = env.observation_space.n, env.action_space.n\n",
    "    for episode in range(episodes):\n",
    "        s = env.reset()\n",
    "        a = epsilon_greedy(Q, epsilon, n_actions, s, exploit_only)\n",
    "        t = 0\n",
    "        current_eng = 0\n",
    "        episode_eng = 0\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while t < max_steps:\n",
    "            t += 1\n",
    "            if exploit_only is True:\n",
    "                s_, reward, done, info = env.step(a[0][0])\n",
    "                # This iterates over the proposed actions\n",
    "                # And calculates if the true action s_\n",
    "                # Is in one of the 5 or 10 proposed\n",
    "                for index, proposed_action in enumerate(a[0]):\n",
    "                    if s_ == proposed_action:\n",
    "                        if index == 0:\n",
    "                            current_eng += 1\n",
    "                        if index < 5:\n",
    "                            hr_at_5 += 1\n",
    "                            ndcg_at_5 += (1.0 / log2(2+index))\n",
    "                        hr_at_10 += 1\n",
    "                        ndcg_at_10 += (1.0 / log2(2+index))\n",
    "                    else:\n",
    "                        pass \n",
    "                if current_eng > episode_eng:\n",
    "                    episode_eng = current_eng              \n",
    "            else:\n",
    "                s_, reward, done, info = env.step(a)\n",
    "\n",
    "            total_reward += reward\n",
    "            a_ = epsilon_greedy(Q, epsilon, n_actions, s_, exploit_only)\n",
    "            if exploit_only is False:\n",
    "                # If we are not in test phase\n",
    "                # This is the Q table update function\n",
    "                if done:\n",
    "                    Q[s, a] += alpha * ( reward  - Q[s, a] )\n",
    "                else:\n",
    "                    Q[s, a] += alpha * ( reward + (gamma * Q[s_, a_]) - Q[s, a] )\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            s, a = s_, a_\n",
    "            if done:\n",
    "                timestep_reward.append(total_reward / t)\n",
    "                epsilon = max(0.0 , np.exp(-exploration_decay_rate / 100 * episode))\n",
    "                eng += episode_eng  \n",
    "                break\n",
    "    return timestep_reward, Q, hr_at_5, hr_at_10, ndcg_at_5, ndcg_at_10, eng"
   ]
  },
  {
   "source": [
    "### Experiment 1: Warmup On 70% + Q Learning\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training\n",
    "Q_bayes, experiment_1_train_df = get_q_warmup(train_click_df, percentage=0.7, max_q_size=max_q_size)\n",
    "env = MooChoose_v1(experiment_1_train_df, window_size=1, possible_items=max_q_size)\n",
    "t, Q, _, _, _, _, _ = qlearning(env=env, Q=Q_bayes, alpha = 1, gamma = 0.5, episodes = 30_000, max_steps = 10, exploration_decay_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.title(\"Average Rewards Per Episode - Q learning with warmup\")\n",
    "plt.plot(t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing\n",
    "env = MooChoose_v1(test_click_df, window_size=1, possible_items=max_q_size)\n",
    "test_episodes = test_click_df['session_id'].max() - test_click_df['session_id'].min() + 1\n",
    "test_clicks = test_click_df.shape[0]\n",
    "p, _, hr_at_5, hr_at_10, ndcg_at_5, ncdg_at_10, eng = qlearning(env=env, Q=Q,  episodes = test_episodes, exploit_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.title(\"Average Rewards Per Episode - Q learning with warmup Test\")\n",
    "plt.plot(p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test results: \\n HR_5: {round(100 *hr_at_5/test_clicks,2)} \\n HR_10: {round(100 *hr_at_10/test_clicks,2)} \\n NDCG_5: {round(100 * ndcg_at_5/test_clicks,2)} \\n NDCG_10: {round( 100*ncdg_at_10/test_clicks,2)} \\n ENG: {round(100*eng/test_clicks,2)}\")"
   ]
  },
  {
   "source": [
    "### Experiment 2: No Warmup Q Learning\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training\n",
    "del Q_bayes\n",
    "del Q\n",
    "\n",
    "\n",
    "Q = init_q(max_q_size, max_q_size, type=\"zeros\")\n",
    "env = MooChoose_v1(train_click_df, window_size=1, possible_items=max_q_size)\n",
    "t, Q, _, _, _, _, _ = qlearning(env=env, Q=Q, alpha = 1, gamma = 0.5, episodes = 30_000, max_steps = 10, exploration_decay_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.title(\"Average Rewards Per Episode - Q learning without warmup\")\n",
    "plt.plot(t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing\n",
    "env = MooChoose_v1(test_click_df, window_size=1, possible_items=max_q_size)\n",
    "\n",
    "test_clicks = test_click_df.shape[0]\n",
    "test_episodes = test_click_df['session_id'].max() - test_click_df['session_id'].min() + 1\n",
    "\n",
    "p, _, hr_at_5, hr_at_10, ndcg_at_5, ncdg_at_10, eng = qlearning(env=env, Q=Q,  episodes=test_episodes, exploit_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.title(\"Average Rewards Per Episode - Q learning without warmup Test\")\n",
    "plt.plot(p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test results: \\n HR_5: {round(100 *hr_at_5/test_clicks,2)} \\n HR_10: {round(100 *hr_at_10/test_clicks,2)} \\n NDCG_5: {round(100 * ndcg_at_5/test_clicks,2)} \\n NDCG_10: {round( 100*ncdg_at_10/test_clicks,2)} \\n ENG: {round(100*eng/test_clicks,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Reinforcement Learning Imports\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pip install keras-rl2 example : https://github.com/wau/keras-rl2/blob/master/examples/dqn_cartpole.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.agents import SARSAAgent\n",
    "from rl.policy import BoltzmannQPolicy, GreedyQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "source": [
    "### Deep Learning Evaluation Methods\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state_to_model_input(state):\n",
    "    state = state.reshape(1, 1,-1)\n",
    "    return state\n",
    "\n",
    "def forward_pass(model, state):\n",
    "    state = convert_state_to_model_input(state)\n",
    "    predictions = model.predict(state)\n",
    "    top_10_predictions = np.argsort(predictions[0])[::-10]\n",
    "    return top_10_predictions\n",
    "    \n",
    "def deep_qlearning_evaluation(env, model, test_episodes):\n",
    "    # Keep a counter on total number of test clicks #\n",
    "    n_clicks = 0\n",
    "\n",
    "    # Hit Rate Metrics # \n",
    "    hr_at_5 = 0\n",
    "    hr_at_10 = 0\n",
    "\n",
    "    # NDCG Metrics #\n",
    "    ndcg_at_5 = 0\n",
    "    ndcg_at_10 = 0\n",
    "\n",
    "    # Engagement Metric #\n",
    "    eng = 0\n",
    "\n",
    "    n_states, n_actions = env.observation_space.n, env.action_space.n\n",
    "    for episode in range(test_episodes):\n",
    "        s = env.reset()\n",
    "        a = forward_pass(model, s)\n",
    "        t = 0\n",
    "        current_eng = 0\n",
    "        episode_eng = 0\n",
    "        done = False\n",
    "        while t < 10:\n",
    "            n_clicks += 1\n",
    "            t += 1\n",
    "            s_, reward, done, info = env.step(a[0])\n",
    "\n",
    "            # This iterates over the proposed actions\n",
    "            # And calculates if the true action s_\n",
    "            # Is in one of the 5 or 10 proposed\n",
    "            for index, proposed_action in enumerate(a):\n",
    "                if s_[-1] == proposed_action:\n",
    "                    if index == 0:\n",
    "                        current_eng += 1\n",
    "                    if index < 5:\n",
    "                        hr_at_5 += 1\n",
    "                        ndcg_at_5 += (1.0 / log2(2+index))\n",
    "                    hr_at_10 += 1\n",
    "                    ndcg_at_10 += (1.0 / log2(2+index))\n",
    "                else:\n",
    "                    pass\n",
    "            if current_eng > episode_eng:\n",
    "                episode_eng = current_eng          \n",
    "            a_ = forward_pass(model, s_)\n",
    "            s, a = s_, a_\n",
    "            if done:\n",
    "                eng += episode_eng\n",
    "                break\n",
    "    return round(100* hr_at_5 / n_clicks, 2), round(100 * hr_at_10 / n_clicks,2), round(100* ndcg_at_5 / n_clicks,2), round(100*ndcg_at_10 / n_clicks,2), round(eng / n_clicks,2)"
   ]
  },
  {
   "source": [
    "### Deep Reinforcement Learning Methods\n",
    "#### Method 1: Deep Sarsa \n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "env = MooChoose_v1(train_click_df, window_size=window_size, possible_items=max_q_size)\n",
    "n_states, n_actions = env.observation_space.n, env.action_space.n\n",
    "\n",
    "model_dsarsa = Sequential()\n",
    "model_dsarsa.add(Flatten(input_shape=(1,) + (window_size,)))\n",
    "model_dsarsa.add(Embedding(n_states+1, 64, input_length=window_size))\n",
    "model_dsarsa.add(LSTM(128, return_sequences=False))\n",
    "model_dsarsa.add(Dense(100, activation='relu'))\n",
    "model_dsarsa.add(Dense(n_actions, activation='linear'))\n",
    "\n",
    "\n",
    "\n",
    "# SARSA does not require a memory.\n",
    "policy = EpsGreedyQPolicy(eps=0.1)\n",
    "sarsa_agent = SARSAAgent(model=model_dsarsa, nb_actions=n_actions, nb_steps_warmup=0, policy=policy)\n",
    "sarsa_agent.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dsarsa.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_sarsa = sarsa_agent.fit(env, nb_steps=30_000, visualize=False, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er = history_sarsa.history['episode_reward']\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Average Rewards Per Episode - Deep Sarsa')\n",
    "plt.plot(er)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "env = MooChoose_v1(test_click_df, window_size=window_size, possible_items=max_q_size)\n",
    "test_episodes = test_click_df['session_id'].max() - test_click_df['session_id'].min() + 1\n",
    "\n",
    "sarsa_agent.test(env, nb_episodes=test_episodes, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract the Agent Model and save it\n",
    "model_dsarsa.save('dsarsa_agent_k500_full.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_at_5, hr_at_10, ndcg_at_5, ndcg_at_10, eng = deep_qlearning_evaluation(env, model_dsarsa, test_episodes)\n",
    "print(f\"Test results: \\n HR_5: {hr_at_5} \\n HR_10: {hr_at_10} \\n NDCG_5: {ndcg_at_5} \\n NDCG_10: {ndcg_at_10} \\n ENG: {eng}\")"
   ]
  },
  {
   "source": [
    "### Deep Reinforcement Learning Methods\n",
    "#### Method 2: Deep Q Learning with Time Window \n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Pretraining Funtions and Data Generation\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding_pretraining_data(train_click_df, window_size, max_q_size):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for f in train_click_df.groupby('session_id')['item_id'].apply(list).values:\n",
    "        need_to_pad = 10 - len(f)\n",
    "        sequence = f + ([max_q_size] * need_to_pad) \n",
    "        \n",
    "        for ii in range(0,  9 - window_size):\n",
    "            train_sequence = sequence[ii:ii+window_size]\n",
    "            label_of_sequence = sequence[ii+window_size+1]\n",
    "            sequences.append(train_sequence)\n",
    "            labels.append(label_of_sequence)\n",
    "    return sequences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = generate_embedding_pretraining_data(train_click_df, 3, max_q_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(sequences)\n",
    "y_train = np.array(labels)"
   ]
  },
  {
   "source": [
    "#### Pretrain Embeddings and Base Network of DQN (NWP task)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "env = MooChoose_v1(train_click_df, window_size=window_size, possible_items=max_q_size)\n",
    "n_states, n_actions = env.observation_space.n, env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=(window_size,))\n",
    "\n",
    "x = Embedding(n_states+1, 64, input_length=window_size, name='Emb_Layer')(model_input)\n",
    "x = LSTM(units=128, return_sequences=False, name='LSTM_Layer')(x)\n",
    "x = Dense(100, activation='relu', name='Dense_Layer')(x)\n",
    "x = Dropout(rate=0.3)(x)\n",
    "model_output = Dense(n_states+1, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[model_input], outputs=[model_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=256, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "source": [
    "#### Get the Embeddings, LSTM and Dense Layer and use them in the Reinforcment Learning Agent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.get_layer('Emb_Layer')\n",
    "lstm_layer = model.get_layer('LSTM_Layer')\n",
    "dense_layer = model.get_layer('Dense_Layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "env = MooChoose_v1(train_click_df, window_size=window_size,possible_items=max_q_size)\n",
    "n_states, n_actions = env.observation_space.n, env.action_space.n\n",
    "\n",
    "memory = SequentialMemory(limit=10_000, window_length=1)\n",
    "\n",
    "model_dqn = Sequential()\n",
    "model_dqn.add(Flatten(input_shape=(1,) + (window_size,)))\n",
    "model_dqn.add(embedding_layer)\n",
    "model_dqn.add(lstm_layer)\n",
    "model_dqn.add(dense_layer)\n",
    "model_dqn.add(Dense(n_actions, activation='linear'))\n",
    "\n",
    "policy = EpsGreedyQPolicy(eps=0.1)\n",
    "dqn = DQNAgent(model=model_dqn, nb_actions=n_actions, policy=policy, memory=memory,\n",
    "               nb_steps_warmup=200, gamma=0.5, target_model_update=500)\n",
    "\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dqn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = dqn.fit(env, nb_steps=30_000, visualize=False, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er = history.history['episode_reward']\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Average Rewards Per Episode - Deep Q Learning with NWP warmup')\n",
    "plt.plot(er)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "env = MooChoose_v1(test_click_df, window_size=window_size, possible_items=max_q_size)\n",
    "test_episodes = test_click_df['session_id'].max() - test_click_df['session_id'].min() + 1\n",
    "\n",
    "l=dqn.test(env, nb_episodes=test_episodes, visualize=False)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract the Agent Model and save it\n",
    "model_dqn.save('dqn_agent_k500_full.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "env = MooChoose_v1(test_click_df, window_size=window_size, possible_items=max_q_size)\n",
    "test_episodes = test_click_df['session_id'].max() - test_click_df['session_id'].min() + 1\n",
    "\n",
    "hr_at_5, hr_at_10, ndcg_at_5, ndcg_at_10, eng = deep_qlearning_evaluation(env, model_dqn, test_episodes)\n",
    "print(f\"Test results: \\n HR_5: {hr_at_5} \\n HR_10: {hr_at_10} \\n NDCG_5: {ndcg_at_5} \\n NDCG_10: {ndcg_at_10} \\n ENG: {eng}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Cuda",
   "language": "python",
   "name": "rapid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}