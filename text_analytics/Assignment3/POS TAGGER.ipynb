{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "from conllu import parse,parse_incr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "from math import floor\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_flat(alist):\n",
    "    flat_list = []\n",
    "    for sublist in alist:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert Conllu files into datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataset(conllu_file):\n",
    "    data_file = open(conllu_file, \"r\", encoding=\"utf-8\")\n",
    "    sentences = []\n",
    "    pos_tags = []\n",
    "    length = []\n",
    "    for tokenlist in parse_incr(data_file):\n",
    "        sentence = [f['lemma'].lower() for f in tokenlist]\n",
    "        tags = [f['upos'] for f in tokenlist]\n",
    "        length.append(len(sentence))\n",
    "        sentences.append(sentence)\n",
    "        pos_tags.append(tags)\n",
    "    \n",
    "    avg_length = floor(np.array(length).mean())\n",
    "    print(f\"Found {len(sentences)} sentences of average size {avg_length} in the dataset!\\n\")\n",
    "    \n",
    "    return sentences, pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4287 sentences of average size 19 in the dataset!\n",
      "\n",
      "Found 784 sentences of average size 19 in the dataset!\n",
      "\n",
      "Found 890 sentences of average size 17 in the dataset!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = convert_to_dataset('gum/train.conllu')\n",
    "dev_data = convert_to_dataset('gum/dev.conllu')\n",
    "test_data = convert_to_dataset('gum/test.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_window_dataset(data, window_size, pad_symbol='</s>'):\n",
    "    z_sentences = data[0]\n",
    "    z_pos_tags = list_flat(data[1])\n",
    "    windowed_sentences = []\n",
    "    for sentence in z_sentences:\n",
    "        gen = ngrams(sentence, window_size, pad_left=True, pad_right=True, left_pad_symbol=pad_symbol, right_pad_symbol=pad_symbol)\n",
    "        for f in list(gen)[window_size//2:-(window_size//2)]:\n",
    "            windowed_sentences.append(f)\n",
    "    return windowed_sentences, z_pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = convert_to_window_dataset(train_data,5)\n",
    "x_dev, y_dev = convert_to_window_dataset(dev_data,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get all known words in train set and make a vocabulary out of them (add special </s> token)\n",
    "train_vocab = set(list_flat(x_train))\n",
    "train_vocab = {v:k for k,v in enumerate(train_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = set(y_train)\n",
    "train_target = {v:k for k,v in enumerate(train_target)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wget.download('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use Gensim to Load the Weights\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(train_vocab), 300))\n",
    "\n",
    "for i in range(len(model.wv.vocab)):\n",
    "    candidate_word = model.wv.index2word[i]\n",
    "    if candidate_word in train_vocab:\n",
    "        embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[train_vocab[candidate_word]] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert train and dev sets\n",
    "x_train_tokenized = []\n",
    "for sentence in x_train:\n",
    "    tokenized_sentence = []\n",
    "    for token in sentence:\n",
    "        if token in train_vocab:\n",
    "            tokenized_sentence.append(train_vocab[token])\n",
    "        else:\n",
    "            tokenized_sentence.append(train_vocab['</s>'])\n",
    "    x_train_tokenized.append(tokenized_sentence)\n",
    "\n",
    "\n",
    "x_dev_tokenized = []\n",
    "for sentence in x_dev:\n",
    "    tokenized_sentence = []\n",
    "    for token in sentence:\n",
    "        if token in train_vocab:\n",
    "            tokenized_sentence.append(train_vocab[token])\n",
    "        else:\n",
    "            tokenized_sentence.append(train_vocab['</s>'])\n",
    "    x_dev_tokenized.append(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert train and dev targets\n",
    "y_train_tokenized = []\n",
    "for label in y_train:\n",
    "    y_train_tokenized.append(train_target[label])\n",
    "    \n",
    "y_dev_tokenized = []\n",
    "for label in y_dev:\n",
    "    y_dev_tokenized.append(train_target[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ready for Model Preprocessing and Feeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Input(shape=(5,),dtype='float32',name='MyInputLayer'))\n",
    "model.add(Embedding(len(train_vocab), 300, weights=[embedding_matrix], input_length=5, trainable=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=200, activation='relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(units=len(train_target), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 5, 300)            2731500   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               300200    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 17)                1717      \n",
      "=================================================================\n",
      "Total params: 3,053,517\n",
      "Trainable params: 3,053,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = np.array(x_train_tokenized)\n",
    "xt = xt.astype('float32')\n",
    "\n",
    "xd = np.array(x_dev_tokenized)\n",
    "xd = xd.astype('float32')\n",
    "\n",
    "\n",
    "yt = np.array(y_train_tokenized)\n",
    "yt = yt.astype('int')\n",
    "\n",
    "yd = np.array(y_dev_tokenized)\n",
    "yd = yd.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom Callbacks\n",
    "class Metrics(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, valid_data):\n",
    "        super(Metrics, self).__init__()\n",
    "        self.validation_data = valid_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        val_predict = np.argmax(self.model.predict(self.validation_data[0]), -1)\n",
    "        val_targ = self.validation_data[1]\n",
    "        if len(val_targ.shape) == 2 and val_targ.shape[1] != 1:\n",
    "            val_targ = np.argmax(val_targ, -1)\n",
    "        val_targ = tf.cast(val_targ,dtype=tf.float32)\n",
    "        \n",
    "\n",
    "        _val_f1 = f1_score(val_targ, val_predict,average=\"weighted\")\n",
    "        _val_recall = recall_score(val_targ, val_predict,average=\"weighted\")\n",
    "        _val_precision = precision_score(val_targ, val_predict,average=\"weighted\")\n",
    "\n",
    "        logs['val_f1'] = _val_f1\n",
    "        logs['val_recall'] = _val_recall\n",
    "        logs['val_precision'] = _val_precision\n",
    "        print(\" — val_f1: %f — val_precision: %f — val_recall: %f\" % (_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = ModelCheckpoint('weights.hdf5',\n",
    " monitor='val_f1', \n",
    " mode='max', verbose=2,\n",
    " save_best_only=True,\n",
    " save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81857 samples\n",
      "81664/81857 [============================>.] - ETA: 0s - loss: 0.0560 — val_f1: 0.901384 — val_precision: 0.903742 — val_recall: 0.900212\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.90138, saving model to weights.hdf5\n",
      "81857/81857 [==============================] - 13s 157us/sample - loss: 0.0561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ed79d3c828>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xt,yt,epochs=200, batch_size=256, callbacks=[Metrics(valid_data=(xd, yd)),cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_to_predict = \"this a dog who run\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_to_predict = [train_vocab[f] for f in phrase_to_predict.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1000, 4447, 8561, 8356, 4977]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_to_predict = np.array([phrase_to_predict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(phrase_to_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': 0,\n",
       " 'NUM': 1,\n",
       " 'VERB': 2,\n",
       " 'PART': 3,\n",
       " 'NOUN': 4,\n",
       " 'ADV': 5,\n",
       " 'SYM': 6,\n",
       " 'AUX': 7,\n",
       " 'ADJ': 8,\n",
       " 'DET': 9,\n",
       " 'INTJ': 10,\n",
       " 'SCONJ': 11,\n",
       " 'PRON': 12,\n",
       " 'ADP': 13,\n",
       " 'PROPN': 14,\n",
       " 'PUNCT': 15,\n",
       " 'CCONJ': 16}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2 Env",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
