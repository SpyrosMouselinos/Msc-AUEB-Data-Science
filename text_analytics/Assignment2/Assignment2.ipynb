{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GHbd7wGM_Bc6",
        "8CncrlPtIs2w",
        "WRNWwP1wLVkR"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydCMp2Pf-wzo",
        "colab_type": "text"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1Ui5cxV-1D2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "48655ce4-1312-4c4c-b188-3403334e775b"
      },
      "source": [
        "!pip install kaggle contractions"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.24)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.4.5.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (from contractions) (0.0.17)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.4.0)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSBcEaHWAdQC",
        "colab_type": "text"
      },
      "source": [
        "### Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ2wOqcxAgKl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "1d6c6a1f-d336-4b4a-efc2-463d58926335"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = 'spyrosmouselinos'\n",
        "os.environ['KAGGLE_KEY'] = 'a907fb69eab07900ccb6e1f2874fd343'\n",
        "\n",
        "import re\n",
        "import contractions\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "from zipfile import ZipFile\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHbd7wGM_Bc6",
        "colab_type": "text"
      },
      "source": [
        "### Connect to Kaggle API and download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LS71Vxh_Gq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "\n",
        "\n",
        "if not os.path.exists('train.csv'):\n",
        "    api.competition_download_file('twitter-sentiment-analysis2','train.csv')\n",
        "    zf = ZipFile('train.csv.zip', 'r')\n",
        "    zf.extractall('./')\n",
        "    zf.close()\n",
        "    os.remove('train.csv.zip')\n",
        "\n",
        "data = pd.read_csv('train.csv', delimiter=',',  encoding='latin-1')\n",
        "data.drop(columns='ItemID', inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxY7Jo3ZAzZC",
        "colab_type": "text"
      },
      "source": [
        "### Text Preprocessing / Splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp9Gzefs-Np_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['SentimentText'] = data['SentimentText'].str.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r-3oN8e-NqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_emojis(sentence):\n",
        "    # Converts known emojis to sentiment words\n",
        "\n",
        "    # :) --> happyface\n",
        "    # (: --> happyface\n",
        "    # :p --> happyface\n",
        "    # :P --> happyface\n",
        "    # ;p --> happyface\n",
        "    # :] --> happyface\n",
        "    # [: --> happyface\n",
        "\n",
        "    # :o --> surpriseface\n",
        "    # :O --> surpriseface\n",
        "\n",
        "    # :( --> sadface\n",
        "    # ): --> sadface\n",
        "    # :'( --> sadface\n",
        "    # :S -->  sadface\n",
        "    # :\\ --> sadface\n",
        "    # :[ --> sadface\n",
        "    # ]: ---> sadface\n",
        "    return\n",
        "\n",
        "\n",
        "def preprocess(sentence):\n",
        "    # Convert to Lower Case\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # Replace Contractions\n",
        "    sentence = contractions.fix(sentence, slang=True)\n",
        "\n",
        "    # Remove Links\n",
        "    sentence = re.sub(r'(http|https|www)\\S+', ' ', sentence)\n",
        "\n",
        "    # Remove usernames\n",
        "    sentence = re.sub(r'@\\w+', ' ', sentence)\n",
        "\n",
        "    # Remove non-word characters\n",
        "    sentence = re.sub(r'\\W', ' ', sentence)\n",
        "\n",
        "    # Remove underscores \n",
        "    sentence = re.sub(r'[-_]', ' ', sentence)\n",
        "\n",
        "    # Remove numbers\n",
        "    sentence = re.sub(r'[0-9]', ' ', sentence)\n",
        "    \n",
        "    # Remove all single characters\n",
        "    sentence = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sentence)\n",
        "\n",
        "    # Substituting multiple spaces with single space\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence, flags=re.I)\n",
        "\n",
        "    # Lemmatization\n",
        "    sentence = sentence.split()\n",
        "\n",
        "    sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
        "    sentence = ' '.join(sentence)\n",
        "    return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT9_njYN-NqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['SentimentText'] = data['SentimentText'].apply(lambda x : preprocess(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3eY0UCK-NqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Convert into lists and split\n",
        "y = data['Sentiment'].values\n",
        "x = data['SentimentText'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q47I6nyc-NqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split  \n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, stratify=y, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CncrlPtIs2w",
        "colab_type": "text"
      },
      "source": [
        "### Features \n",
        "* TF\n",
        "* TF-IDF\n",
        "\n",
        "### Classifiers\n",
        "* Dummy\n",
        "* Logistic Regression\n",
        "* Logistic Regression SGD\n",
        "* Naive Bayes\n",
        "* KNN\n",
        "* SVM \n",
        "* MLP\n",
        "* Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAULAJOmN3Be",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC as SVM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRNWwP1wLVkR",
        "colab_type": "text"
      },
      "source": [
        "### Dummy Classifier (Input/Tuning is Irrelevant) ✔"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlrtsecXIyVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make classifier\n",
        "base = DummyClassifier(strategy='most_frequent')\n",
        "\n",
        "# Fit on Train Data\n",
        "base.fit(np.zeros_like(y_train), y_train)\n",
        "\n",
        "# Make Predictions on Training Set\n",
        "predictions = base.predict(np.zeros_like(y_train))\n",
        "\n",
        "score = accuracy_score(y_train, predictions)\n",
        "print(\"train accuracy: %.2f%%\" % (score*100))\n",
        "\n",
        "# Make Predictions on Test Set\n",
        "predictions_test = base.predict(np.zeros_like(y_test))\n",
        "score = accuracy_score(y_test, predictions_test)\n",
        "print(\"test accuracy: %.2f%%\" % (score*100))\n",
        "\n",
        "\n",
        "print(\"Test data confusion matrix\")\n",
        "y_true = pd.Series(y_test, name='True')\n",
        "y_pred = pd.Series(predictions_test, name='Predicted')\n",
        "pd.crosstab(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tyo8KmsL8Hl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsmGbyOGNbo0",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNMyFoQ-NjVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(min_df=10, max_features=5000, stop_words=stopwords.words('english'))),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('dim_reduction', TruncatedSVD()),\n",
        "    ('clf', LogisticRegression(class_weight='balanced'))\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    'vect__max_df': (0.6, 0.7),\n",
        "    'vect__ngram_range': ((1,2),(1,3)),\n",
        "    'dim_reduction__n_components': (100, 500, 1000),\n",
        "    'tfidf__use_idf': (True, False),\n",
        "    'tfidf__norm': ('l1', 'l2'),\n",
        "    'clf__C': (1.0, 0.1, 0.01),\n",
        "}\n",
        "\n",
        "clf = GridSearchCV(pipeline, parameters, scoring='f1', cv=3, n_jobs=-1)\n",
        "clf.fit(x_train, y_train)\n",
        "print(\"Best Score: \", clf.best_score_)\n",
        "print(\"Best Params: \", clf.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6pyBWp7SA24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Results of Grid Search\n",
        "# Best Score:  0.762361919024294\n",
        "# Best Params:  {'clf__C': 1.0, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__max_df': 0.7, 'vect__min_df': 0.0, 'vect__ngram_range': (1, 3)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRVh8rIfWpoc",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to-PuGH7WrzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(min_df=10, max_features=5000,stop_words=stopwords.words('english'))),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('dim_reduction', TruncatedSVD()),\n",
        "    ('clf', SGDClassifier(loss=\"log\", max_iter=1000, class_weight='balanced'))\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    'vect__max_df': (0.6, 0.7),\n",
        "    'vect__ngram_range': ((1,2), (1,3)),  \n",
        "    'dim_reduction__n_components': (100,500,1000),\n",
        "    'tfidf__use_idf': (True, False),\n",
        "    'tfidf__norm': ('l1', 'l2'),\n",
        "    'clf__penalty':('l1','l2'),\n",
        "    'clf__alpha':(0.01, 0.001, 0.0001)\n",
        "}\n",
        "\n",
        "clf = GridSearchCV(pipeline, parameters, scoring='f1', cv=3, n_jobs=-1)\n",
        "clf.fit(x_train, y_train)\n",
        "print(\"Best Score: \", clf.best_score_)\n",
        "print(\"Best Params: \", clf.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IItP3pOVYHty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Best Score:  0.7605481382673948\n",
        "# Best Params:  {'clf__alpha': 0.0001, 'clf__penalty': 'elasticnet', 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__max_df': 1.0, 'vect__min_df': 0.0, 'vect__ngram_range': (1, 2)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyGFFsE1YwB-",
        "colab_type": "text"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djCuX7-HYYZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(min_df=10, max_features=5000, stop_words=stopwords.words('english'))),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('dim_reduction', TruncatedSVD()),\n",
        "    ('clf', MultinomialNB(fit_prior=True))\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    'vect__max_df': (0.6, 0.7),\n",
        "    'vect__ngram_range': ((1, 2), (1,3)),\n",
        "    'dim_reduction__n_components': (100,500,1000),\n",
        "    'tfidf__use_idf': (True, False),\n",
        "    'tfidf__norm': ('l1', 'l2'),\n",
        "    'clf__alpha': (1.0, 0.1, 0.01)\n",
        "}\n",
        "\n",
        "clf = GridSearchCV(pipeline, parameters, scoring='f1', cv=3, n_jobs=-1)\n",
        "clf.fit(x_train, y_train)\n",
        "print(\"Best Score: \", clf.best_score_)\n",
        "print(\"Best Params: \", clf.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bwe-jaMwZixJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPzqCsYwZjNE",
        "colab_type": "text"
      },
      "source": [
        "### KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMRG-LSoZqNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(min_df=10,max_features=5000,stop_words=stopwords.words('english'))),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('dim_reduction', TruncatedSVD()),\n",
        "    ('clf', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    'vect__max_df': (0.6, 0.7),\n",
        "    'vect__ngram_range': ((1, 2), (1,3)),  \n",
        "    'dim_reduction__n_components': (100,500,1000), \n",
        "    'tfidf__use_idf': (True, False),\n",
        "    'tfidf__norm': ('l1', 'l2'),\n",
        "    'clf__n_neighbors': (3,5,7)\n",
        "}\n",
        "\n",
        "clf = GridSearchCV(pipeline, parameters, scoring='f1', cv=3, n_jobs=-1)\n",
        "clf.fit(x_train, y_train)\n",
        "print(\"Best Score: \", clf.best_score_)\n",
        "print(\"Best Params: \", clf.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBQlESJabgjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEV5gdPubg-K",
        "colab_type": "text"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcZXkpN4b12l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(min_df=10, max_features=5000, stop_words=stopwords.words('english'))),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('dim_reduction', TruncatedSVD()),\n",
        "    ('clf', SVM())\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    'vect__max_df': (0.6, 0.7),\n",
        "    'vect__ngram_range': ((1, 2), (1,3)),  \n",
        "    'tfidf__use_idf': (True, False),\n",
        "    'dim_reduction__n_components': (100,500,1000),\n",
        "    'tfidf__norm': ('l1', 'l2'),\n",
        "    'clf__C':(1.0, 0.1, 0.01),\n",
        "    'clf__kernel':('linear','sigmoid')\n",
        "}\n",
        "\n",
        "clf = GridSearchCV(pipeline, parameters, scoring='f1', cv=3, n_jobs=-1)\n",
        "clf.fit(x_train, y_train)\n",
        "print(\"Best Score: \", clf.best_score_)\n",
        "print(\"Best Params: \", clf.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuin2JW1dh-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}