{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment I: Scraping, Twitter API & Pandas\n",
    "## Name: Mouselinos Spyridon \n",
    "## Date: October 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Scraping Twitter Accounts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Connect to Twitter\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For the rest of the exercise we have successfully created a twitter account and stored our credentials/key-pairs in a file named twitter_config.py\n",
    "### For the sake of this exercise the file is located in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import time\n",
    "from twitter_config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to Establish Connection to Twitter Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def establish_connection(config):\n",
    "    auth = tweepy.OAuthHandler(config['consumer_key'], config['consumer_secret'])\n",
    "    auth.set_access_token(config['access_token'], config['access_token_secret'])\n",
    "    try:\n",
    "        api = tweepy.API(auth)\n",
    "    except:\n",
    "        raise(\"Connection Not Established...\")\n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = establish_connection(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the most trendy topics for Athens and print the first 10 (HINT: woeid 946738)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to return N first Trends of a WOEID Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_n_trends_for_location(api, woeid, first_n_results):\n",
    "    trends_list = api.trends_place(id=woeid)[0]['trends'][0:first_n_results]\n",
    "    for trend in trends_list:\n",
    "        print(trend['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_n_trends_for_location(api=api, woeid=946738,first_n_results=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Username Scraping\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape 10 usernames from [the 49 best Twitter accounts to follow in UK politics](https://www.businessinsider.com/uk-politics-twitter-accounts-2016-8?r=US&IR=T#48-matt-singh-2) and put them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We will need to create a BS4 Scraper that will be used both in Question 1.2 as well as all following scraping Questions.\n",
    "### The scrapper will take as input the link to scrap as well as the optional boolean argument selenium.\n",
    "### For the sake of completeness i decided to add the scraper full functionality for both non-js / js loaded webpages.\n",
    "### Note to user: In case the selenium flag is set to True, a path to the geckodriver executable is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_soup(link, selenium=False, path_to_driver=r'C:\\\\Program Files\\\\GeckoDriver\\\\geckodriver.exe'):\n",
    "    if selenium:\n",
    "            options = Options()\n",
    "            # We dont need an actual Firefox window to be opened.\n",
    "            options.headless = True\n",
    "            driver = webdriver.Firefox(options=options, executable_path=path_to_driver)\n",
    "            # We wait a little so the webpage is fully loaded. I used 30 seconds for this example.\n",
    "            driver.implicitly_wait(30)\n",
    "            driver.get(link)\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    else:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "        except requests.exceptions.MissingSchema as ms:\n",
    "            # Missing URL schema\n",
    "            print(ms)\n",
    "        except requests.exceptions.ConnectionError as ce:\n",
    "            # Connection error\n",
    "            print(ce)\n",
    "        except requests.exceptions.HTTPError as herror:\n",
    "            # Invalid HTTP response\n",
    "            print(herror)\n",
    "        except requests.exceptions.Timeout as toerr:\n",
    "            # Timeout error\n",
    "            print(toerr)\n",
    "        else:\n",
    "            print(\"Page retrieval OK\")\n",
    "            soup =  BeautifulSoup(r.content, 'html.parser')    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now lets solve the exercize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = retrieve_soup(\"https://www.businessinsider.com/uk-politics-twitter-accounts-2016-8?r=US&IR=T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Real Names are pretty easy as they are hidden in the h2 Tag ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = [f.text.split('. ')[-1] for f in soup.find_all(\"h2\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The respective Twitter Usernames are in the class \"slide-layout clearfix\" as the first <a> tag ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs = soup.find_all(\"div\", {\"class\":{\"slide-layout clearfix\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Some of theese strings contain the \\xa0 \n",
    "### which needs to be removed explicitly as mentioned [here](https://stackoverflow.com/questions/10993612/python-removing-xa0-from-string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_username_list = [('@' + f.find('p').get_text().split('@')[-1].replace(u'\\xa0', u'')) for f in divs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now that we have the twiiter username list lets perform a sanity check\n",
    "assert len(twitter_username_list) == len(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now lets get the top 10 of them in order to put them in a List\n",
    "top_10_list = twitter_username_list[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Fetch Tweets\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy, datetime, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fetch the tweets posted in the last 24 hours for each of the usernames.\n",
    "\n",
    "* Use `try`/`except` to bypass any one not present (e.g., deleted ones) and print \"Cound not fetch @username\" (where `username` is the name of the respective user).\n",
    "\n",
    "* Use `datetime`, `timedelta` from `datetime` to define the last day.\n",
    "\n",
    "* Use `tweepy.Cursor`, fetching no more than 100 tweets at a time.\n",
    "\n",
    "* Use `api.user_timeline` to fetch the tweets, and check the Tweeter API on how [timelines](https://developer.twitter.com/en/docs/tweets/timelines/guides/working-with-timelines) work.\n",
    "\n",
    "* Print the number of tweets you fetched for each username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The limit Handler you provided in lectures notes ##\n",
    "\n",
    "def limit_handler(cursor):\n",
    "    while True:\n",
    "        try:\n",
    "            yield cursor.next()\n",
    "            # This is to avoid running into a rate limit\n",
    "            time.sleep(60)\n",
    "        except tweepy.RateLimitError:\n",
    "            # But if we do run into a rate limit, then\n",
    "            # go to sleep for 15 minutes\n",
    "            print('Rate limit reached')\n",
    "            time.sleep(15 * 60)\n",
    "        except tweepy.TweepError as te:\n",
    "            if te[0]['code'] == '419':\n",
    "                print('Requests limit reached')\n",
    "                time.sleep(15 * 60)\n",
    "            else:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions ##\n",
    "def check_tweet_time(tweet):\n",
    "    ## Checks if a tweet happened until 24h ago.\n",
    "    dt = (datetime.datetime.now() - tweet.created_at)\n",
    "    if dt < datetime.timedelta(days=1):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def check_if_retweet_or_reply(tweet):\n",
    "    if not tweet.retweeted and tweet.in_reply_to_status_id is None:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(api, screen_name):\n",
    "    \n",
    "    # Flag to Mark results from a specific ID and backwards to read.\n",
    "    # Defaults to the most recent id. This avoids re-reading data due\n",
    "    # to the adding of recent tweets to the tweet stack.\n",
    "    max_id = None\n",
    "\n",
    "    # Flag to Mark results from a specific ID and forward to read.\n",
    "    # Defaults to None so the API can go as far back as it can.\n",
    "    since_id = None\n",
    "    \n",
    "    ## The total number of collected tweets ##\n",
    "    n_tweets = 0\n",
    "    \n",
    "    cursor = tweepy.Cursor(api.user_timeline, count=100, screen_name=screen_name).pages()\n",
    "    \n",
    "    page_n = 0\n",
    "    for page in limit_handler(cursor):\n",
    "        for tweet in page:\n",
    "            if page_n == 0:\n",
    "                since_id = tweet.id\n",
    "            ## If the tweet is time eligible ##\n",
    "            if check_tweet_time(tweet): \n",
    "                ## If the tweet is not a retweet count it ##\n",
    "                if check_if_retweet_or_reply(tweet):\n",
    "                    n_tweets += 1\n",
    "            else:\n",
    "                ## Update the max_id to the last seen tweet id ##\n",
    "                max_id = tweet.id -1\n",
    "                print(\"User {} had {} tweets in the past 24h\".format(screen_name, n_tweets))\n",
    "                return since_id, max_id, n_tweets\n",
    "        ## Update the Page Number ##\n",
    "        page_n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_tweets(api, screen_name, since_id, max_id, n_tweets):\n",
    "    \n",
    "    ## We will look only in the 1st page because come on how many tweets could one do in mere seconds? ##\n",
    "    cursor = tweepy.Cursor(api.user_timeline, count=100, since_id=str(since_id), max_id=str(max_id), screen_name=screen_name).pages(1)\n",
    "    \n",
    "    latest_tweets = 0 \n",
    "    \n",
    "    for page in limit_handler(cursor):\n",
    "        for tweet in page:\n",
    "            ## If the tweet is time eligible ##\n",
    "            if check_tweet_time(tweet): \n",
    "                ## If the tweet is not a retweet count it ##\n",
    "                if check_if_retweet_or_reply(tweet):\n",
    "                    latest_tweets += 1                                \n",
    "            else:\n",
    "                print(\"User {} had just made {} more tweets at a grand total of {}\".format(screen_name,latest_tweets,latest_tweets+n_tweets))\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We will keep a dictionary with the last and first processed tweets of each elibigle user ###\n",
    "### Then perform a pass on them in order to find new tweets that happened while we processed the other users ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_dict = {}\n",
    "for tun in twitter_username_list:\n",
    "    try:\n",
    "        ## If user exists ##\n",
    "        api.get_user(tun)\n",
    "        ## Get his/her tweets ##\n",
    "        since_id, max_id, n_tweets = get_tweets(api=api, screen_name=tun)\n",
    "        retry_dict.update(\n",
    "            {\n",
    "                tun:[since_id, max_id, n_tweets]\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        print(\"Could not fetch tweets for user: {}\".format(tun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tun in retry_dict.keys():\n",
    "    since_id = retry_dict[tun][0]\n",
    "    max_id = retry_dict[tun][1]\n",
    "    n_tweets = retry_dict[tun][2]\n",
    "    try:\n",
    "        get_latest_tweets(api=api, screen_name=tun, since_id=since_id, max_id=max_id, n_tweets=n_tweets)\n",
    "    except:\n",
    "        print(\"No new tweets for user: {}\".format(tun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Processing Twitter accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Scrape and Put in a `DataFrame`\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape the Twitter accounts of all the UK parliament members and put them in a `DataFrame`; you can get use the following resource: <https://www.mpsontwitter.co.uk/list>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We will use our function retrieve_soup that waas created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = retrieve_soup(\"https://www.mpsontwitter.co.uk/list\", selenium=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will fetch the table body from the table object that holds our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find('tbody',{'id':'mp_wrapper'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## By inspecting the table we notice that the total number of accounts is 587 at the moment, however in order to be safe we keep a margin of 600 and then drop the N/A rows\n",
    "## so we take advantage of that by inserting it as an index during the Frame Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name the DataFrame columns as `name`, `username`, `constituency`, `party`, `num_followers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_dataframe = pd.DataFrame(columns=['name', 'username', 'constituency', 'party', 'num_followers'], index=range(0,600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_marker = 0\n",
    "### For every row in the Table Body:\n",
    "for row in table.find_all('tr'):\n",
    "    \n",
    "    column_marker = 0\n",
    "    ### Find all Columns\n",
    "    columns = row.find_all('td')\n",
    "    ### And then for every column that we need,\n",
    "    ### Meaning columns 3-5:\n",
    "    ### Add it to the frame using our index\n",
    "    for idx, column in enumerate(columns):\n",
    "        if (idx < 2) or (idx > 6):\n",
    "            continue\n",
    "        mp_dataframe.iat[row_marker,column_marker] = column.get_text()\n",
    "        ## Increase the column index\n",
    "        column_marker += 1\n",
    "    ## Increase the row index\n",
    "    row_marker +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_dataframe.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure that the followers_num is shown as a number, not a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Curate num_followers into a number ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_dataframe['num_followers'] = mp_dataframe['num_followers'].apply(lambda x: int(x.replace(\",\", \"\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create a Hierarchical Index\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `party` will be the top level and `followers_num` will be the next.\n",
    "\n",
    "#### Show only the `username` column (apart from the index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We can do this in 1 line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_dataframe.set_index(['party','num_followers'])['username']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Assess the Party Tweeter Power\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We group the frame by the party column and aggregate the num_followers column by the sum function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = mp_dataframe.groupby('party').num_followers.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Then we plot it by BarPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In order to plot it in seaborn we have to do a trick first\n",
    "### A good way i thought of was the pivot table method following the reset index so we drop the extra column created by the pivoting\n",
    "### Then the data are in the format needed by seaborn to be plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = mp_dataframe.pivot_table(index='party', aggfunc='sum')\n",
    "pivot_df.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.title(\"Bar plot of the tweeter power of each party\")\n",
    "chart = sns.barplot(x='party', y='num_followers',data=pivot_df)\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=45)\n",
    "# Overwrite the Default Violin Plot axes\n",
    "plt.xlabel(\"Party\")\n",
    "plt.ylabel(\"Number of Followers\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Processing Text Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Prepare a Profanity Set\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url):\n",
    "    \n",
    "    ###    Uses requests to get a remote file,\n",
    "    ###    saves it in chunks and renames it \n",
    "    ###    according to the last '/' part of the link.\n",
    "    \n",
    "    get_response = requests.get(url,stream=True)\n",
    "    file_name  = url.split(\"/\")[-1]\n",
    "    with open(file_name, 'wb') as f:\n",
    "        for chunk in get_response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = download('http://staffwww.dcs.shef.ac.uk/people/G.Gorrell/publications-materials/abuse-terms.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have the file abuse-terms.txt in our folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets filter out the wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we open the file\n",
    "with open(file_name, 'r') as f:\n",
    "   # Read the file contents and generate a list with each line\n",
    "   lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 25 lines are junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines[24:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bad_word_finder(lines):\n",
    "    bad_words = set()\n",
    "    pattern = re.compile(r'[a-zA-Z]+[\\s\\w-]+[\\n\\t]+')\n",
    "    # Iterate each line\n",
    "    for line in lines:\n",
    "        # Regex applied to each line \n",
    "        match = re.search(pattern, line)\n",
    "        if match:\n",
    "            # Drop the last char (\\n or \\t)\n",
    "            bad_words.add(match.group()[:-1].lower())\n",
    "    return bad_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = bad_word_finder(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Parse Tweets\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read tweets from <https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv> and rename the `tweet` column to `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(url, usecols=['count','hate_speech','offensive_language','neither','class','tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={\"tweet\": \"text\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert all of them to lower for avoiding mismatching errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data.text.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new column with the list of words of the each text, placed in a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['word_list'] = data.text.str.findall(r'[a-zA-Z]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We dont need text column anymore\n",
    "del data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Count Abuse\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets find out the number of bad words per entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['n_bad_words'] = data['word_list'].apply(set).apply(lambda x: len(x & bad_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### And then the bad words themselves in each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['bad_words'] = data['word_list'].apply(set).apply(lambda x: list(x & bad_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We dont need the word lists any more\n",
    "del data['word_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### We create a statistics dictionary that holds information about each required statistic to be returned\n",
    "#### This serves as a placeholder to be filled in afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = {\n",
    "    0 : {\n",
    "        'minimum' : None,\n",
    "        'maximum' : None,\n",
    "        'mean': None, \n",
    "        'median': None,\n",
    "        'sum': None\n",
    "    },\n",
    "    \n",
    "    1 : {\n",
    "        'minimum' : None,\n",
    "        'maximum' : None,\n",
    "        'mean': None, \n",
    "        'median': None,\n",
    "        'sum': None\n",
    "    },\n",
    "    \n",
    "    2 : {\n",
    "        'minimum' : None,\n",
    "        'maximum' : None,\n",
    "        'mean': None, \n",
    "        'median': None,\n",
    "        'sum': None\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_id, frame in data.groupby(by='class'):\n",
    "    statistics[class_id]['minimum'] = frame['n_bad_words'].min()\n",
    "    statistics[class_id]['maximum'] = frame['n_bad_words'].max()\n",
    "    statistics[class_id]['mean'] = frame['n_bad_words'].mean()\n",
    "    statistics[class_id]['median'] = frame['n_bad_words'].median()\n",
    "    statistics[class_id]['sum'] = frame['n_bad_words'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the mean, median, minimum, maximum, and sum of bad words in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Visualize Profanity \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 DistPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dist Plot does not support data separation by hue so we have to do it manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hint for this solution was found here: https://stackoverflow.com/questions/46045750/python-distplot-with-multiple-distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the DataFrame into 3 class based Dataframes\n",
    "class_0_df = data.loc[data['class'] == 0]\n",
    "class_1_df = data.loc[data['class'] == 1]\n",
    "class_2_df = data.loc[data['class'] == 2]\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.xlabel(\"Number of Bad Words\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distibution Plot of Bad Words in Each Class\")\n",
    "sns.distplot(class_0_df[['n_bad_words']], hist=False, label='Class 0')\n",
    "sns.distplot(class_1_df[['n_bad_words']], hist=False, label='Class 1')\n",
    "sns.distplot(class_2_df[['n_bad_words']], hist=False, label='Class 2')\n",
    "plt.legend(ncol=3, fontsize=15)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 ViolinPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.title(\"Violin Plot of Bad Words in Each Class\")\n",
    "sns.violinplot(x=\"class\", y=\"n_bad_words\", data=data, hue=\"class\")\n",
    "# Overwrite the Default Violin Plot axes\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Density\")\n",
    "# Overwrite the Default Legend Size and Shape\n",
    "plt.legend(ncol=3, fontsize=15)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3 BoxPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.title(\"Box Plot of Bad Words in Each Class\")\n",
    "sns.boxplot(x=\"class\", y=\"n_bad_words\", data=data, hue=\"class\")\n",
    "# Overwrite the Default Violin Plot axes\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Density\")\n",
    "# Overwrite the Default Legend Size and Shape\n",
    "plt.legend(ncol=3, fontsize=15)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4 StripPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.title(\"Box Plot of Bad Words in Each Class\")\n",
    "sns.stripplot(x=\"class\", y=\"n_bad_words\", data=data, hue=\"class\")\n",
    "# Overwrite the Default Violin Plot axes\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Density\")\n",
    "# Overwrite the Default Legend Size and Shape\n",
    "plt.legend(ncol=3, fontsize=15)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Explore Profanity per Class\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we need to reduce the total bad words to 3 lists one for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import functools\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove rows without bad words\n",
    "data = data.loc[data['n_bad_words'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bad_words = {\n",
    "    0: None,\n",
    "    1: None,\n",
    "    2: None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_30 = {\n",
    "    0: None,\n",
    "    1: None,\n",
    "    2: None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Fastest way to convert a list of lists into a single list\n",
    "#### See: https://stackoverflow.com/a/45323085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_bad_words_for_class(series):\n",
    "    def functools_reduce_iconcat(a):\n",
    "        return functools.reduce(operator.iconcat, a, [])\n",
    "    return functools_reduce_iconcat(list(series.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_n_words(words,n=30):\n",
    "    return Counter(words).most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_id, frame in data.groupby(by='class'):\n",
    "    total_bad_words[class_id] = collect_bad_words_for_class(frame['bad_words'])\n",
    "    top_30[class_id] = find_top_n_words(total_bad_words[class_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,3):\n",
    "    print(\"Top 30 for class: {}\".format(i))\n",
    "    print(top_30[i])\n",
    "    print(\"*---------------------*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that a lot of theese words stem from the same word, maybe we could create a mapping to merge the similar ones and recalculate the top 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectify_dict = {\n",
    "    'nigga'   : 'nigger',\n",
    "    'niggers' : 'nigger',\n",
    "    'negro'   : 'nigger',\n",
    "    'niglet'  : 'nigger',\n",
    "    'fag'     : 'faggot',\n",
    "    'bitches' : 'bitch',\n",
    "    'pussies' : 'pussy',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply it to \"all words collected\" dictionaries and recalculate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,3):\n",
    "    total_bad_words[i] = [f if f not in rectify_dict.keys() else rectify_dict[f] for f in total_bad_words[i]]\n",
    "    top_30[i] = find_top_n_words(total_bad_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,3):\n",
    "    print(\"Top 30 for class: {}\".format(i))\n",
    "    print(top_30[i])\n",
    "    print(\"*---------------------*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Why not create a nice word count blob for visualization ##\n",
    "## Warning: this part of the exercize needs the library wordcloud  to be installed ##\n",
    "## I made the following line to be installed from the notebook if needed ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "except:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install wordcloud\n",
    "    print(\"Might need to reload Jupyter Kernel to Continue...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code inspired from https://www.pythoncircle.com/post/689/python-script-16-generating-word-cloud-image-of-a-text-using-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# image configurations\n",
    "background_color = \"#101010\"\n",
    "height = 720\n",
    "width = 1080\n",
    "\n",
    "\n",
    "for class_n in range(0,3):\n",
    "    print(\"Class: {}\".format(class_n))\n",
    "    word_cloud = WordCloud(\n",
    "        background_color=background_color,\n",
    "        width=width,\n",
    "        height=height\n",
    "    )\n",
    "\n",
    "    word_cloud.generate_from_frequencies(dict(top_30[class_n]))\n",
    "    # Display the generated image:\n",
    "    plt.imshow(word_cloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF.13-Gpu",
   "language": "python",
   "name": "tf.13-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
