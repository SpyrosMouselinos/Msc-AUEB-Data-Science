{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Link_Prediction_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9AElynhoIOr"
      },
      "source": [
        "### <font color='#ff6600'>General Imports</font>  \n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8-skh16RMih",
        "outputId": "9c7ab2aa-17e8-4d17-bc76-61701bc400a3"
      },
      "source": [
        "import csv\n",
        "import time\n",
        "import string\n",
        "import torch\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from random import randint\n",
        "from urllib.request import urlopen\n",
        "import random\n",
        "import re\n",
        "import math\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "from termcolor import cprint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_7YWOLbvBKp",
        "outputId": "385f5cdc-0594-497c-cebb-b294238e5820"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRJzhIxOoMzh"
      },
      "source": [
        "### <font color='#ff6600'>Model Imports</font>  \n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x7f8yexU-aa"
      },
      "source": [
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier \n",
        "from sklearn.model_selection import GridSearchCV \n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.decomposition import TruncatedSVD, PCA\n",
        "from sklearn.preprocessing import RobustScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KRhjXNeu-kp"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETwPQ9dCaD2H"
      },
      "source": [
        "### <font color='#ff3399'>Text Pre - Processing Functions</font>  \n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUCgw2buaA2z"
      },
      "source": [
        "def preprocess_abstracts(abstract):\n",
        "    # Skip last character (newline)\n",
        "    abstract = abstract[:-1]\n",
        "\n",
        "    # Convert to Lower Case\n",
        "    abstract = abstract.lower()\n",
        "\n",
        "    # Remove non-word characters\n",
        "    abstract = re.sub(r'\\W', ' ', abstract)\n",
        "\n",
        "    # Remove underscores \n",
        "    abstract = re.sub(r'[-_]', ' ', abstract)\n",
        "\n",
        "    # Remove numbers\n",
        "    abstract = re.sub(r'[0-9]', ' ', abstract)\n",
        "    \n",
        "    # Remove all single characters\n",
        "    abstract = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', abstract)\n",
        "\n",
        "    # Substituting multiple spaces with single space\n",
        "    abstract = re.sub(r'\\s+', ' ', abstract, flags=re.I)\n",
        "\n",
        "    # Lemmatization\n",
        "    abstract = abstract.split()\n",
        "    abstract = [lemmatizer.lemmatize(word) for word in abstract]\n",
        "\n",
        "    # Remove Stop Words\n",
        "    abstract = [w for w in abstract if not w in stop_words]\n",
        "\n",
        "    abstract = ' '.join(abstract)\n",
        "\n",
        "    return abstract"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlvSUDD_fxId"
      },
      "source": [
        "def preprocess_authors(author_names):\n",
        "    # Skip last character (newline)\n",
        "    author_names = author_names[:-1]\n",
        "\n",
        "    # Split on comma\n",
        "    possible_authors = author_names.split(',')\n",
        "\n",
        "    # Remove non-word characters\n",
        "    possible_authors = [re.sub(r'\\W', ' ', f) for f in possible_authors]\n",
        "\n",
        "    # Substituting multiple spaces with single space\n",
        "    possible_authors = [re.sub(r'\\s+', ' ', f, flags=re.I) for f in possible_authors]\n",
        "   \n",
        "    # Keep first letter of first name + whole last name\n",
        "    return_authors = []\n",
        "    for author in possible_authors:\n",
        "        author = author.split(' ')\n",
        "        author = [f for f in author if f != '' and f != ' ' and f != '  ']\n",
        "        # Case 1: Surname Only\n",
        "        if len(author) == 1:\n",
        "            author = author[0]\n",
        "        elif len(author) > 1:\n",
        "            first_name_chars = ''.join([f[0] for f in author[0:-1]])\n",
        "            author = first_name_chars + ' ' + author[-1]\n",
        "        else:\n",
        "            raise ValueError(\"Zero Length Name!\")\n",
        "        return_authors.append(author.lower())\n",
        "    return return_authors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMnqwjppey02"
      },
      "source": [
        "### <font color='#751aff'>Get Edgelist</font> \n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3BaOMnYYw1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e334a3e-9d2b-4e48-90fc-2462110b930e"
      },
      "source": [
        "# Create a graph\n",
        "G = nx.read_edgelist(urlopen('http://www.db-net.aueb.gr/nikolentzos/data_challenge_2021/edgelist.txt'), delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
        "nodes = list(G.nodes())\n",
        "n = G.number_of_nodes()\n",
        "m = G.number_of_edges()\n",
        "print('Number of nodes:', n)\n",
        "print('Number of edges:', m)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of nodes: 138499\n",
            "Number of edges: 1091955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcVWwvMbmkm6"
      },
      "source": [
        "### <font color='#751aff'>Get Abstracts</font>\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0Phz-Rjd2CH"
      },
      "source": [
        "# Read the abstract of each paper\n",
        "abstracts = dict()\n",
        "corpus = []\n",
        "data = urlopen('http://www.db-net.aueb.gr/nikolentzos/data_challenge_2021/abstracts.txt')\n",
        "for line in data:\n",
        "    decoded_line = line.decode(\"utf-8\")\n",
        "    node, abstract = decoded_line.split('|--|')\n",
        "    preprocessed_abstract = preprocess_abstracts(abstract)\n",
        "    corpus.append(preprocessed_abstract)\n",
        "    abstracts[int(node)] = preprocessed_abstract.split(' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9gDRMocRwYB"
      },
      "source": [
        "##### <font color='#b380ff'>TF-IDF Feature Creation</font> \n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bif3pneYfjqV"
      },
      "source": [
        "# Create TF-IDF Vectors (cosine similarity solution)\n",
        "vectorizer =  TfidfVectorizer(min_df=0.0, max_df=0.5,\n",
        "                              sublinear_tf=True, ngram_range=(1,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPNsvGIrg-XN"
      },
      "source": [
        "abstract_corpus = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIIKfkj_R34B"
      },
      "source": [
        "##### <font color='#b380ff'>SVD as Topic Modeling on TF-IDF vectors</font> \n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmwYZI_Mi8fo"
      },
      "source": [
        "dechonker = TruncatedSVD(n_components=200, random_state=1995)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6buX8QejpIx",
        "outputId": "fa24b606-ecf3-4043-bb55-d4e2015bdc95"
      },
      "source": [
        "dechonker.fit(abstract_corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TruncatedSVD(algorithm='randomized', n_components=200, n_iter=5,\n",
              "             random_state=1995, tol=0.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JadcL4xtmAeE"
      },
      "source": [
        "abstract_corpus_svd = dechonker.transform(abstract_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0-bO7hs7usu"
      },
      "source": [
        "# Map text to set of terms (set word solution)\n",
        "for node in abstracts:\n",
        "    abstracts[node] = set(abstracts[node])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tv69t4Mmmw5"
      },
      "source": [
        "### <font color='#751aff'>Get Authors</font>\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axiSxdy5mpuB"
      },
      "source": [
        "# Read the abstract of each paper\n",
        "authors = dict()\n",
        "data = urlopen('http://www.db-net.aueb.gr/nikolentzos/data_challenge_2021/authors.txt')\n",
        "for line in data:\n",
        "    decoded_line = line.decode(\"utf-8\")\n",
        "    node, author = decoded_line.split('|--|')\n",
        "    authors[int(node)] = preprocess_authors(author_names=author)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWMXj5aBnB15"
      },
      "source": [
        "# Map text to set of terms\n",
        "for node in authors:\n",
        "    authors[node] = set(authors[node])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlF8ynHKC9Yr"
      },
      "source": [
        "### <font color='#006699'>Feature Generation Functions</font>\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dr_O7RMh6ys"
      },
      "source": [
        "####   🔥  <font color='#006699'>General Functions</font>\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIBNSFQKC_OC"
      },
      "source": [
        "def fast_cosine_similarity(vec_a, vec_b, use_norm=False):\n",
        "    if use_norm:\n",
        "        norm_vec_a = np.linalg.norm(vec_a, ord=2)\n",
        "        norm_vec_b = np.linalg.norm(vec_b, ord=2)\n",
        "        if norm_vec_a > 0:\n",
        "            vec_a = vec_a / norm_vec_a\n",
        "        if norm_vec_b > 0:\n",
        "            vec_b = vec_b / norm_vec_b\n",
        "\n",
        "    return cosine_similarity(vec_a, vec_b)[0][0]\n",
        "\n",
        "def get_2nd_deg(item, neighbour_set):\n",
        "    tempset = set()\n",
        "    for f in neighbour_set:\n",
        "        tempset = tempset.union(set(G.adj[f]))\n",
        "    tempset.remove(item)\n",
        "    return tempset\n",
        "\n",
        "def calc_jaccard(seta, setb):\n",
        "    intersection = len(seta.intersection(setb))\n",
        "    union = len(seta.union(setb))\n",
        "    return intersection / (union + 1)\n",
        "\n",
        "def calc_adar_index(G, neighbour_set):\n",
        "    index = 0\n",
        "    if len(neighbour_set) == 0:\n",
        "        return 0\n",
        "    else: \n",
        "        for neighbour in neighbour_set:\n",
        "            adar = 1 / math.log(len(G.adj[neighbour]) + 1)\n",
        "            index += adar\n",
        "        return index\n",
        "\n",
        "def smart_short_path(G, n1, n2):\n",
        "    ### Warning Remove the edge first ###\n",
        "    existed = True\n",
        "    try:\n",
        "        G.remove_edge(n1, n2)\n",
        "    except:\n",
        "        existed = False\n",
        "    \n",
        "    ### Now calculate the shortest exclusive path\n",
        "    try:\n",
        "        score = 10 / nx.shortest_path_length(G, n1, n2)\n",
        "    except:\n",
        "        score = 0\n",
        "    \n",
        "    ### Add the edge back if it existed ###\n",
        "    if existed:\n",
        "        G.add_edge(n1,n2)\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YotieBimHCsS"
      },
      "source": [
        "####  😈 <font color='#006699'>Feature: Node ID</font>\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lznisf3HB4l"
      },
      "source": [
        "def log_id_proximity(node_1_id, node_2_id):\n",
        "    node_id_distance = np.abs(node_1_id - node_2_id)\n",
        "    log_node_id_distance = np.log(node_id_distance + 1e-20)\n",
        "    return log_node_id_distance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8swjIVhtq6xS"
      },
      "source": [
        "#### 🚶🏻‍♀️  <font color='#006699'>DeepWalk Algorithm</font>\n",
        "***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci07UtP3q9fe"
      },
      "source": [
        "class DeepWalker():\n",
        "    def __init__(self, allow_dup=False, num_walks=5, walk_length=10):\n",
        "        self.allow_dup = allow_dup\n",
        "        self.walk_length = walk_length\n",
        "        self.num_walks = num_walks\n",
        "        self.saved_walks = []\n",
        "    \n",
        "    def get_random_walk(self, G, node):\n",
        "        walk = [node]\n",
        "        for _ in range(self.walk_length-1):\n",
        "            neighbors = list(G.neighbors(walk[-1]))\n",
        "            target_node = random.choice(neighbors)\n",
        "            if not self.allow_dup:\n",
        "                retries = 0\n",
        "                while target_node in walk:\n",
        "                    target_node = random.choice(neighbors)\n",
        "                    retries += 1\n",
        "                    if retries > 5:\n",
        "                        break\n",
        "            walk.append(target_node)\n",
        "        walk = [str(f) for f in walk]\n",
        "        return walk\n",
        "    \n",
        "\n",
        "    def start_walking(self, G):\n",
        "        nodes = list(G.nodes())\n",
        "        for _ in range(self.num_walks):\n",
        "            idx = np.random.permutation(len(nodes))\n",
        "            for i in range(len(nodes)):\n",
        "                node = nodes[idx[i]]\n",
        "                walk = self.get_random_walk(G, node)\n",
        "                self.saved_walks.append(walk)\n",
        "        return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK2Km5DRkAwU"
      },
      "source": [
        "def deepwalk(G, n_dim):\n",
        "    print(\"Generating walks\")\n",
        "    johny = DeepWalker()\n",
        "    johny.start_walking(G)\n",
        "    walks = johny.saved_walks\n",
        "\n",
        "    print(\"Training word2vec\")\n",
        "    model = Word2Vec(vector_size=n_dim, window=8, min_count=0, sg=1, workers=8)\n",
        "    model.build_vocab(walks)\n",
        "    model.train(walks, total_examples=model.corpus_count, epochs=5)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWEoN_w6kEK-"
      },
      "source": [
        "n_dim = 64\n",
        "model = deepwalk(G, n_dim) \n",
        "\n",
        "embeddings = np.zeros((n, n_dim))\n",
        "for node in G.nodes():\n",
        "    embeddings[node,:] = model.wv[str(node)] / (np.linalg.norm(model.wv[str(node)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFPfKhi10qhc"
      },
      "source": [
        "def get_deepwalk_similarity(embeddings, node_a, node_b):\n",
        "    return np.dot(embeddings[node_a,:], embeddings[node_b,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXHrGBzdH7DO"
      },
      "source": [
        "####  🕵🏻 <font color='#006699'>Unlinked Neighbors Finder</font>\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr0zmMfPIMMx"
      },
      "source": [
        "def find_unlinked_neighbors(margin=20):\n",
        "    \"\"\"\n",
        "        Instead of totaly random node pair, pick a close neighbour\n",
        "        that does not have a link with the n1_node.\n",
        "    \"\"\"\n",
        "    n1_ = nodes[randint(0, n-1)]\n",
        "    n2_ = nodes[min(n-1,max(0,randint(n1_ - margin, n1_ + margin)))]\n",
        "\n",
        "    while n2_ in set(G.adj[n1_]):\n",
        "        margin = margin * 2\n",
        "        n2_ = nodes[min(n-1,max(0,randint(n1_ - margin, n1_ + margin)))]\n",
        "    return n1_, n2_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIsc2j4mrgwe"
      },
      "source": [
        "## <font color='#006699'>Text features</font>\n",
        "* Number of common terms - Feature [8]\n",
        "* Number of common terms normalized - Feature [9]\n",
        "* Number of non-common terms - Feature [10]\n",
        "* Number of non-common terms normalized - Feature [11]\n",
        "* Number of common authors - Feature [12]\n",
        "* Number of common authors' last names - Feature [13]\n",
        "* Number of non-common authors - Feature [14]\n",
        "* Number of non-common authors' last names - Feature [15]\n",
        "* Cosine similarity between tfidf vectors (uni/bigram level) of abstracts - Feature [16]\n",
        "* Cosine similarity between svd vectors of abstracts - Feature [17]\n",
        "\n",
        "## <font color='#006699'>Graph features</font>\n",
        "\n",
        "* Sum of Degrees of Nodes - Feaure [1]\n",
        "* Difference of Degrees of Nodes - Feature [2]\n",
        "* Sum of 2nd Degree of Nodes - Feature [3]\n",
        "* Difference of 2nd Degree of Nodes - Feature [4]\n",
        "* Jaccard Index 1, 2 - Features [5,6]\n",
        "* Adamic Adar Index 1 - Features [7, 18]\n",
        "* Shortest Path Distance - Feature [19]\n",
        "* DeepWalk Similarity - Feature [20]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVLelJ-awcdT"
      },
      "source": [
        "def get_features(X_table, y_table, n1, n2, idx, y_mark=None):\n",
        "    # Graph Features\n",
        "    neigh_1d_0 = set(G.adj[n1])\n",
        "    neigh_1d_1 = set(G.adj[n2])\n",
        "\n",
        "    neigh_2d_0 = get_2nd_deg(n1, neigh_1d_0)\n",
        "    neigh_2d_1 = get_2nd_deg(n2, neigh_1d_1)\n",
        "\n",
        "    degree_1d_0 = len(neigh_1d_0)\n",
        "    degree_1d_1 = len(neigh_1d_1)\n",
        "\n",
        "    degree_2d_0 = len(neigh_2d_0)\n",
        "    degree_2d_1 = len(neigh_2d_1)\n",
        "\n",
        "    jaccard_1d = calc_jaccard(neigh_1d_0, neigh_1d_1)\n",
        "    jaccard_2d = calc_jaccard(neigh_2d_0, neigh_2d_1)\n",
        "\n",
        "    adar_1d = calc_adar_index(G, list(neigh_1d_0.intersection(neigh_1d_1)))\n",
        "    adar_2d = calc_adar_index(G, list(neigh_2d_0.intersection(neigh_2d_1)))\n",
        "\n",
        "    # Text Features\n",
        "    abstracts_0 = abstracts[n1]\n",
        "    abstracts_1 = abstracts[n2]\n",
        "    authors_0 = authors[n1]\n",
        "    authors_0_ln = set([f.split(' ')[-1] for f in authors_0])\n",
        "    authors_1 = authors[n2]\n",
        "    authors_1_ln = set([f.split(' ')[-1] for f in authors_1])\n",
        "\n",
        "    n_terms_0 = len(abstracts_0)\n",
        "    n_terms_1 = len(abstracts_1)\n",
        "    n_common_terms = len(abstracts_0.intersection(abstracts_1))\n",
        "    n_common_terms_norm = n_common_terms / (n_terms_0 + n_terms_1)\n",
        "    n_uncommon_terms = abs(len(abstracts_0.difference(abstracts_1)))\n",
        "    n_uncommon_terms_norm = n_uncommon_terms / (n_terms_0 + n_terms_1)\n",
        "\n",
        "    n_common_authors = len(authors_0.intersection(authors_1))\n",
        "    n_common_authors_last_name = len(authors_0_ln.intersection(authors_1_ln))\n",
        "\n",
        "    n_uncommon_authors = abs(len(authors_0.difference(authors_1)))\n",
        "    n_uncommon_authors_last_name = abs(len(authors_0_ln.difference(authors_1_ln)))\n",
        "\n",
        "    pure_cos_sim = fast_cosine_similarity(abstract_corpus[n1].reshape(1,-1), abstract_corpus[n2].reshape(1,-1))\n",
        "    svd_cos_sim = fast_cosine_similarity(abstract_corpus_svd[n1].reshape(1,-1), abstract_corpus_svd[n2].reshape(1,-1), use_norm=True)\n",
        "\n",
        "    shortest_path = smart_short_path(G, n1, n2)\n",
        "\n",
        "    ### Plug them in\n",
        "    X_table[idx,0] = degree_1d_0 + degree_1d_1\n",
        "    X_table[idx,1] = abs(degree_1d_0 - degree_1d_1)\n",
        "    X_table[idx,2] = degree_2d_0 + degree_2d_1\n",
        "    X_table[idx,3] = abs(degree_2d_0 - degree_2d_1)\n",
        "    X_table[idx,4] = jaccard_1d\n",
        "    X_table[idx,5] = jaccard_2d\n",
        "    X_table[idx,6] = adar_1d\n",
        "\n",
        "    X_table[idx,7] = n_common_terms\n",
        "    X_table[idx,8] = n_common_terms_norm\n",
        "    X_table[idx,9] = n_uncommon_terms\n",
        "    X_table[idx,10] = n_uncommon_terms_norm\n",
        "\n",
        "    X_table[idx,11] = n_common_authors\n",
        "    X_table[idx,12] = n_common_authors_last_name\n",
        "    X_table[idx,13] = n_uncommon_authors\n",
        "    X_table[idx,14] = n_uncommon_authors_last_name\n",
        "\n",
        "    X_table[idx, 15] = pure_cos_sim\n",
        "    X_table[idx, 16] = svd_cos_sim\n",
        "    X_table[idx, 17] = adar_2d\n",
        "    X_table[idx, 18] = shortest_path\n",
        "    X_table[idx, 19] = log_id_proximity(n1,n2)\n",
        "\n",
        "    if y_mark is None:\n",
        "        return\n",
        "    else:\n",
        "        y_table[idx] = y_mark\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdIPZOitbeb1"
      },
      "source": [
        "## <font color='#00b38f'>Collecting Data </font>\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgbTmNzSc-7Q"
      },
      "source": [
        "def negative_sampling(G:nx.Graph, start, end, validation_split:float=0.0):\n",
        "    m = end - start\n",
        "    X_train = np.zeros((2*m, 20))\n",
        "    y_train = np.zeros(2*m)\n",
        "    start_time = time.time()\n",
        "    edge_list = list(G.edges())\n",
        "    random.shuffle(edge_list)\n",
        "    for i,edge in enumerate(edge_list):\n",
        "        if i < start:\n",
        "            continue\n",
        "        if i < end:\n",
        "            get_features(X_train, y_train, edge[0], edge[1], 2*(i-start), 1)\n",
        "            n1_, n2_ = find_unlinked_neighbors(margin=20)\n",
        "            get_features(X_train, y_train, n1_, n2_, 2*(i-start)+1, 0)\n",
        "            if (i-start) % 5_000 == 0 and (i-start) != 0:\n",
        "                checkpoint = time.time()\n",
        "                cprint(f\"{round(100*i/m,2)} done in {round(checkpoint - start_time,3)} seconds\", 'cyan')\n",
        "                np.save(f'/content/drive/MyDrive/LinkPrediction/X_train_zesti_{2*(i-start)+1}.npy', X_train[0:2*(i-start)+1,:])\n",
        "                np.save(f'/content/drive/MyDrive/LinkPrediction/y_train_zesti_{2*(i-start)+1}.npy', y_train[0:2*(i-start)+1])\n",
        "        else:\n",
        "            break\n",
        "    np.save(f'/content/drive/MyDrive/LinkPrediction/X_train_zesti_{2*(i-start)+1}.npy', X_train[0:2*(i-start)+1,:])\n",
        "    np.save(f'/content/drive/MyDrive/LinkPrediction/y_train_zesti_{2*(i-start)+1}.npy', y_train[0:2*(i-start)+1])\n",
        "    \n",
        "    if validation_split > 0 and validation_split < 1:\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_split, stratify=y_train, random_state=1995)\n",
        "        print('Size of training matrix:', X_train.shape)\n",
        "        print('Size of validation matrix:', X_val.shape)\n",
        "        return X_train, y_train, X_val, y_val\n",
        "    else:\n",
        "        return X_train, y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNMgf0Bi08jo",
        "outputId": "6ae1f0d1-319d-454c-deae-88fd8a6d3ca3"
      },
      "source": [
        "X_train_full, y_train_full = negative_sampling(G, start=0, end=50_000, validation_split=0.0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[36m10.0 done in 280.417 seconds\u001b[0m\n",
            "\u001b[36m20.0 done in 537.537 seconds\u001b[0m\n",
            "\u001b[36m30.0 done in 813.937 seconds\u001b[0m\n",
            "\u001b[36m40.0 done in 1094.074 seconds\u001b[0m\n",
            "\u001b[36m50.0 done in 1374.884 seconds\u001b[0m\n",
            "\u001b[36m60.0 done in 1651.255 seconds\u001b[0m\n",
            "\u001b[36m70.0 done in 1914.317 seconds\u001b[0m\n",
            "\u001b[36m80.0 done in 2171.373 seconds\u001b[0m\n",
            "\u001b[36m90.0 done in 2438.563 seconds\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrNtMzuWnHp3"
      },
      "source": [
        "## <font color='#00b38f'>Training Models and evaluating on 20% Validation Split </font>\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGcHLIHgr8cS"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, stratify=y_train_full, random_state=1995)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dfm1f0ckKKE"
      },
      "source": [
        "####  🦖 <font color='#b35900'>Logistic Regression </font>  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7Gz6olfi6W8",
        "outputId": "9f6c032e-b798-4558-ba08-435e3282c805"
      },
      "source": [
        "clf = LogisticRegression(max_iter=10_000)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_val)\n",
        "print(classification_report(y_val, y_pred))\n",
        "y_proba = clf.predict_proba(X_val)\n",
        "logloss = log_loss(y_val, y_proba)\n",
        "print(f\"Assumed LogLoss: {logloss}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.96      0.96     10000\n",
            "         1.0       0.96      0.97      0.96     10000\n",
            "\n",
            "    accuracy                           0.96     20000\n",
            "   macro avg       0.96      0.96      0.96     20000\n",
            "weighted avg       0.96      0.96      0.96     20000\n",
            "\n",
            "Assumed LogLoss: 0.103292722327971\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKQV_Fx09IKE"
      },
      "source": [
        "#### 🐆 <font color='#e6b800'> XGBoost "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ot46yCOhYAuv"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4BpwfCcZc6m"
      },
      "source": [
        "best_clf_ = XGBClassifier(max_depth=5, n_estimators=250, learning_rate=0.1, \n",
        " min_child_weight=5, gamma=0.1, subsample=0.6, colsample_bytree=0.6,\n",
        " objective= 'binary:logistic', scale_pos_rate=1, seed=1995, reg_lambda=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA0F6N2AY-DH"
      },
      "source": [
        "best_clf = BaggingClassifier(base_estimator=best_clf_, n_estimators=5, random_state=1995, bootstrap=False).fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgjvfV6XFINK",
        "outputId": "e3dd9992-c456-4418-8239-3be381232f58"
      },
      "source": [
        "y_pred = best_clf.predict(X_val)\n",
        "print(classification_report(y_val, y_pred))\n",
        "y_proba = best_clf.predict_proba(X_val)\n",
        "logloss = log_loss(y_val, y_proba)\n",
        "print(f\"Assumed LogLoss: {logloss}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98     10000\n",
            "         1.0       0.97      0.98      0.98     10000\n",
            "\n",
            "    accuracy                           0.98     20000\n",
            "   macro avg       0.98      0.98      0.98     20000\n",
            "weighted avg       0.98      0.98      0.98     20000\n",
            "\n",
            "Assumed LogLoss: 0.061279525939888946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkomotxh3rju"
      },
      "source": [
        "#### 🦄 <font color='#33ccff'> Deep Learning - MLP \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TubNfuhU1m_O"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiPuFFk9YSbG"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwHR6hNcYdYD"
      },
      "source": [
        "scaler = RobustScaler()\n",
        "X_train_ = scaler.fit_transform(X_train)\n",
        "X_val_ = scaler.transform(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndp9bcXM7ghk",
        "outputId": "0f68b4a5-8569-4a5b-984c-0d3cd4223705"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Input(shape=(X_train_.shape[1],)))\n",
        "for i in range(0,4):\n",
        "    model.add(Dense(units=200))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "model.add(Dense(units=2, activation='softmax'))\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss=CategoricalCrossentropy(label_smoothing=0.001), metrics=['accuracy'])\n",
        "callbacks = EarlyStopping(patience=10, restore_best_weights=True)\n",
        "model.fit(X_train_, to_categorical(y_train), batch_size=256, epochs=300, callbacks=callbacks, validation_data=(X_val_, to_categorical(y_val)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Epoch 1/300\n",
            "313/313 [==============================] - 3s 6ms/step - loss: 0.1234 - accuracy: 0.9542 - val_loss: 0.0809 - val_accuracy: 0.9700\n",
            "Epoch 2/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0933 - accuracy: 0.9664 - val_loss: 0.0761 - val_accuracy: 0.9728\n",
            "Epoch 3/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0898 - accuracy: 0.9672 - val_loss: 0.0756 - val_accuracy: 0.9730\n",
            "Epoch 4/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0866 - accuracy: 0.9687 - val_loss: 0.0731 - val_accuracy: 0.9746\n",
            "Epoch 5/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0845 - accuracy: 0.9688 - val_loss: 0.0716 - val_accuracy: 0.9748\n",
            "Epoch 6/300\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0822 - accuracy: 0.9706 - val_loss: 0.0710 - val_accuracy: 0.9748\n",
            "Epoch 7/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0822 - accuracy: 0.9704 - val_loss: 0.0701 - val_accuracy: 0.9753\n",
            "Epoch 8/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0803 - accuracy: 0.9710 - val_loss: 0.0709 - val_accuracy: 0.9755\n",
            "Epoch 9/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0783 - accuracy: 0.9719 - val_loss: 0.0684 - val_accuracy: 0.9753\n",
            "Epoch 10/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0781 - accuracy: 0.9724 - val_loss: 0.0689 - val_accuracy: 0.9755\n",
            "Epoch 11/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0776 - accuracy: 0.9721 - val_loss: 0.0698 - val_accuracy: 0.9748\n",
            "Epoch 12/300\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0766 - accuracy: 0.9729 - val_loss: 0.0674 - val_accuracy: 0.9757\n",
            "Epoch 13/300\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0750 - accuracy: 0.9732 - val_loss: 0.0685 - val_accuracy: 0.9753\n",
            "Epoch 14/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0758 - accuracy: 0.9730 - val_loss: 0.0704 - val_accuracy: 0.9741\n",
            "Epoch 15/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0744 - accuracy: 0.9731 - val_loss: 0.0679 - val_accuracy: 0.9760\n",
            "Epoch 16/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0731 - accuracy: 0.9745 - val_loss: 0.0682 - val_accuracy: 0.9754\n",
            "Epoch 17/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0719 - accuracy: 0.9744 - val_loss: 0.0673 - val_accuracy: 0.9758\n",
            "Epoch 18/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0734 - accuracy: 0.9738 - val_loss: 0.0666 - val_accuracy: 0.9761\n",
            "Epoch 19/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0719 - accuracy: 0.9744 - val_loss: 0.0653 - val_accuracy: 0.9767\n",
            "Epoch 20/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0727 - accuracy: 0.9739 - val_loss: 0.0684 - val_accuracy: 0.9751\n",
            "Epoch 21/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0713 - accuracy: 0.9747 - val_loss: 0.0669 - val_accuracy: 0.9760\n",
            "Epoch 22/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0715 - accuracy: 0.9747 - val_loss: 0.0668 - val_accuracy: 0.9755\n",
            "Epoch 23/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0695 - accuracy: 0.9753 - val_loss: 0.0661 - val_accuracy: 0.9768\n",
            "Epoch 24/300\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0699 - accuracy: 0.9756 - val_loss: 0.0680 - val_accuracy: 0.9754\n",
            "Epoch 25/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0699 - accuracy: 0.9752 - val_loss: 0.0672 - val_accuracy: 0.9760\n",
            "Epoch 26/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0695 - accuracy: 0.9754 - val_loss: 0.0660 - val_accuracy: 0.9768\n",
            "Epoch 27/300\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0695 - accuracy: 0.9753 - val_loss: 0.0676 - val_accuracy: 0.9762\n",
            "Epoch 28/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0678 - accuracy: 0.9762 - val_loss: 0.0652 - val_accuracy: 0.9772\n",
            "Epoch 29/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0681 - accuracy: 0.9759 - val_loss: 0.0668 - val_accuracy: 0.9765\n",
            "Epoch 30/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0681 - accuracy: 0.9759 - val_loss: 0.0677 - val_accuracy: 0.9761\n",
            "Epoch 31/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0688 - accuracy: 0.9755 - val_loss: 0.0659 - val_accuracy: 0.9762\n",
            "Epoch 32/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0681 - accuracy: 0.9756 - val_loss: 0.0663 - val_accuracy: 0.9768\n",
            "Epoch 33/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0668 - accuracy: 0.9761 - val_loss: 0.0657 - val_accuracy: 0.9765\n",
            "Epoch 34/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0665 - accuracy: 0.9762 - val_loss: 0.0648 - val_accuracy: 0.9774\n",
            "Epoch 35/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0666 - accuracy: 0.9765 - val_loss: 0.0652 - val_accuracy: 0.9768\n",
            "Epoch 36/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0669 - accuracy: 0.9763 - val_loss: 0.0660 - val_accuracy: 0.9765\n",
            "Epoch 37/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0662 - accuracy: 0.9765 - val_loss: 0.0675 - val_accuracy: 0.9761\n",
            "Epoch 38/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0666 - accuracy: 0.9770 - val_loss: 0.0647 - val_accuracy: 0.9768\n",
            "Epoch 39/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0662 - accuracy: 0.9770 - val_loss: 0.0656 - val_accuracy: 0.9764\n",
            "Epoch 40/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0664 - accuracy: 0.9765 - val_loss: 0.0652 - val_accuracy: 0.9771\n",
            "Epoch 41/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0654 - accuracy: 0.9765 - val_loss: 0.0641 - val_accuracy: 0.9776\n",
            "Epoch 42/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0653 - accuracy: 0.9775 - val_loss: 0.0637 - val_accuracy: 0.9768\n",
            "Epoch 43/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0653 - accuracy: 0.9766 - val_loss: 0.0662 - val_accuracy: 0.9767\n",
            "Epoch 44/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0641 - accuracy: 0.9769 - val_loss: 0.0656 - val_accuracy: 0.9769\n",
            "Epoch 45/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0646 - accuracy: 0.9771 - val_loss: 0.0644 - val_accuracy: 0.9773\n",
            "Epoch 46/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0642 - accuracy: 0.9767 - val_loss: 0.0641 - val_accuracy: 0.9766\n",
            "Epoch 47/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0644 - accuracy: 0.9773 - val_loss: 0.0650 - val_accuracy: 0.9771\n",
            "Epoch 48/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0642 - accuracy: 0.9774 - val_loss: 0.0663 - val_accuracy: 0.9763\n",
            "Epoch 49/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0642 - accuracy: 0.9767 - val_loss: 0.0653 - val_accuracy: 0.9772\n",
            "Epoch 50/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0635 - accuracy: 0.9773 - val_loss: 0.0663 - val_accuracy: 0.9768\n",
            "Epoch 51/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0639 - accuracy: 0.9777 - val_loss: 0.0657 - val_accuracy: 0.9763\n",
            "Epoch 52/300\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0640 - accuracy: 0.9774 - val_loss: 0.0651 - val_accuracy: 0.9767\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f13686e6690>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3loHe9W8QqI",
        "outputId": "15a1a886-ba8e-4ca8-8553-3192205cf0f7"
      },
      "source": [
        "y_pred = np.argmax(model.predict(X_val_),axis=1)\n",
        "print(classification_report(y_val, y_pred))\n",
        "y_proba_3 = model.predict(X_val_)\n",
        "logloss = log_loss(y_val, y_proba_3)\n",
        "print(f\"Assumed LogLoss: {logloss}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98     10000\n",
            "         1.0       0.97      0.98      0.98     10000\n",
            "\n",
            "    accuracy                           0.98     20000\n",
            "   macro avg       0.98      0.98      0.98     20000\n",
            "weighted avg       0.98      0.98      0.98     20000\n",
            "\n",
            "Assumed LogLoss: 0.060630760368736755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtdTYSgfoRXP"
      },
      "source": [
        "## <font color='#00bbff'>Get Test Data and Perform Ensemble Prediction\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ4R91fMppID"
      },
      "source": [
        "\n",
        "node_pairs = list()\n",
        "data = urlopen('http://www.db-net.aueb.gr/nikolentzos/data_challenge_2021/test.txt')\n",
        "for line in data:\n",
        "    decoded_line = line.decode(\"utf-8\")\n",
        "    t = decoded_line.split(',')\n",
        "    node_pairs.append((int(t[0]), int(t[1])))\n",
        "\n",
        "# Create the test matrix. Use the same 15 features as above\n",
        "X_test = np.zeros((len(node_pairs), 21))\n",
        "for i,node_pair in enumerate(node_pairs):\n",
        "    get_features(X_test, None, node_pair[0], node_pair[1], i, None)\n",
        "\n",
        "print('Size of test matrix:', X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDtZYNTaBZUA"
      },
      "source": [
        "best_clf = BaggingClassifier(base_estimator=best_clf_, n_estimators=5, random_state=1995, bootstrap=False).fit(X_train_full, y_train_full)\n",
        "y_pred_1 = best_clf.predict_proba(X_test)[:,1]\n",
        "\n",
        "\n",
        "# Write predictions to a file\n",
        "predictions = zip(range(len(y_pred_1)), y_pred_1)\n",
        "with open(\"results.csv\",\"w\") as pred:\n",
        "    csv_out = csv.writer(pred)\n",
        "    csv_out.writerow(['id','predicted'])\n",
        "    for row in predictions:\n",
        "        csv_out.writerow(row) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1j8VAzKKS0d"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}